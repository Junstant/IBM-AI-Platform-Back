# ğŸ¯ Plan de ImplementaciÃ³n RAG - Embeddings Dedicados

## âŒ Problemas Identificados

1. **Modelo inadecuado**: Gemma-2B/Mistral-7B son modelos generativos (Decoder-only), NO son Ã³ptimos para embeddings
2. **ConfusiÃ³n de roles**: Se usa el mismo modelo para generar texto Y para crear embeddings
3. **Falsa selecciÃ³n de modelo**: El cÃ³digo siempre usa Gemma-2B independiente de la selecciÃ³n del usuario
4. **DimensiÃ³n incorrecta**: 4096 dimensiones es excesivo y lento para bÃºsqueda vectorial
5. **Sin especializaciÃ³n**: No hay un modelo dedicado exclusivamente a embeddings

## âœ… SoluciÃ³n ArquitectÃ³nica

### SeparaciÃ³n de Responsabilidades

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FLUJO RAG CORRECTO                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. INGESTA DE DOCUMENTOS (Upload PDF)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Usuario  â”‚â”€â”€â”€â–¶â”‚ RAG API  â”‚â”€â”€â”€â–¶â”‚  Nomic   â”‚â”€â”€â”€â–¶â”‚  Milvus  â”‚
   â”‚ PDF/DOCX â”‚    â”‚ Extract  â”‚    â”‚ Embed    â”‚    â”‚ Vector   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ + Chunk  â”‚    â”‚ (768dim) â”‚    â”‚ Database â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

2. CONSULTA (Query)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Usuario  â”‚â”€â”€â”€â–¶â”‚ RAG API  â”‚â”€â”€â”€â–¶â”‚  Nomic   â”‚â”€â”€â”€â–¶â”‚  Milvus  â”‚â”€â”€â”€â–¶â”‚ Mistral  â”‚
   â”‚ Pregunta â”‚    â”‚          â”‚    â”‚ Embed    â”‚    â”‚ Search   â”‚    â”‚ Generate â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚          â”‚    â”‚ Query    â”‚    â”‚ Top-K    â”‚    â”‚ Response â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Roles Definidos

| Modelo | Rol | Puerto | DimensiÃ³n | PropÃ³sito |
|--------|-----|--------|-----------|-----------|
| **Nomic Embed Text v1.5** | Bibliotecario | 8090 | 768 | Crear embeddings (vectorizaciÃ³n) |
| **Mistral 7B** | Escritor | 8088 | - | Generar respuestas con contexto |
| **Gemma 2B/4B/12B** | Opcionales | 8085-87 | - | Alternativas para generaciÃ³n |

## ğŸ“‹ FASE 1: Descarga del Modelo Nomic

### 1.1 Actualizar model-downloader en docker-compose.yaml

**Archivo**: `docker-compose.yaml`
**SecciÃ³n**: `model-downloader.command`

```yaml
# Agregar DESPUÃ‰S de Deepseek:
# Nomic Embed Text v1.5 - Modelo especializado en embeddings
NOMIC_PATH="/models/nomic-embed-text-v1.5.Q4_K_M.gguf";
if [ ! -f "$$NOMIC_PATH" ]; then
  echo "Modelo Nomic Embed Text v1.5 no encontrado. Descargando...";
  wget --header="Authorization: Bearer $$TOKEN_HUGGHINGFACE" \
    "https://huggingface.co/nomic-ai/nomic-embed-text-v1.5-GGUF/resolve/main/nomic-embed-text-v1.5.Q4_K_M.gguf" \
    -O "$$NOMIC_PATH";
  echo "âœ… Modelo Nomic Embed descargado (274 MB).";
else
  echo "âœ… Modelo Nomic Embed ya existe.";
fi
```

**Status**: â³ Pendiente

---

## ğŸ“‹ FASE 2: Servicio Dedicado para Embeddings

### 2.1 Crear embeddings-api en docker-compose.yaml

**Archivo**: `docker-compose.yaml`
**UbicaciÃ³n**: DespuÃ©s de `deepseek-8b`

```yaml
#^ ======================== EMBEDDINGS API (NOMIC) ========================
embeddings-api:
  image: quay.io/daniel_casali/llama.cpp-mma:v8
  container_name: embeddings-api
  user: "root"
  restart: always
  ports:
    - "${EMBEDDINGS_PORT:-8090}:8080"
  volumes:
    - models_volume:/models
  networks:
    - ai_platform_network
  # COMANDO CRÃTICO: --embedding y --pooling mean para generar embeddings
  command: [
    "--host", "0.0.0.0",
    "--port", "8080",
    "-m", "/models/nomic-embed-text-v1.5.Q4_K_M.gguf",
    "--embedding",           # Modo embedding
    "--pooling", "mean",     # Pooling estrategia
    "-c", "2048",            # Contexto
    "-b", "2048"             # Batch size
  ]
  depends_on:
    model-downloader:
      condition: service_completed_successfully
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
    interval: 30s
    timeout: 10s
    retries: 3
    start_period: 120s
  deploy:
    resources:
      limits:
        cpus: '2.0'
        memory: 1G
```

**Status**: â³ Pendiente

---

## ğŸ“‹ FASE 3: Actualizar ConfiguraciÃ³n RAG

### 3.1 Actualizar rag/config.py

**Cambios crÃ­ticos**:

```python
# Embeddings Service (DEDICADO - Nomic)
EMBEDDING_SERVICE_HOST = os.getenv("EMBEDDING_SERVICE_HOST", "embeddings-api")
EMBEDDING_SERVICE_PORT = os.getenv("EMBEDDING_SERVICE_PORT", "8080")
EMBEDDING_MODEL = "nomic-embed-text-v1.5"
EMBEDDING_DIMENSION = 768  # âš ï¸ CRÃTICO: Nomic usa 768, NO 4096
EMBEDDING_MAX_TOKENS = 8192

# LLM Service (SEPARADO - Para generaciÃ³n)
LLM_HOST = os.getenv("LLM_HOST", "mistral-7b")  # Usar Mistral por defecto
LLM_PORT = os.getenv("LLM_PORT", "8080")
DEFAULT_LLM_MODEL = "mistral-7b"  # Mistral mejor que Gemma

# Document Processing (ajustado para Nomic)
CHUNK_SIZE = 512  # Ã“ptimo para Nomic
CHUNK_OVERLAP = 64
```

**Status**: â³ Pendiente

### 3.2 Actualizar rag/embeddings.py

**Problema actual**: Usa `/embedding` endpoint con procesamiento por lotes manual

**SoluciÃ³n**: Usar `/v1/embeddings` de llama.cpp con --embedding activado

```python
def _post_embedding(self, texts: List[str]) -> List[List[float]]:
    """Llamar a API de embeddings usando endpoint estÃ¡ndar OpenAI"""
    try:
        # Usar endpoint OpenAI-compatible de llama.cpp con --embedding
        payload = {
            "input": texts,  # Puede ser lista o string
            "model": self.model
        }
        
        response = requests.post(
            f"{self.endpoint}/v1/embeddings",
            json=payload,
            headers={"Content-Type": "application/json"},
            timeout=120
        )
        response.raise_for_status()
        
        data = response.json()
        embeddings = [item['embedding'] for item in data['data']]
        
        # Validar dimensiÃ³n
        if embeddings and len(embeddings[0]) != config.EMBEDDING_DIMENSION:
            logger.warning(f"âš ï¸ DimensiÃ³n recibida: {len(embeddings[0])}, esperada: {config.EMBEDDING_DIMENSION}")
        
        return [np.array(e, dtype=np.float32).tolist() for e in embeddings]
        
    except Exception as e:
        logger.error(f"âŒ Error generando embeddings: {e}")
        raise
```

**Status**: â³ Pendiente

---

## ğŸ“‹ FASE 4: Actualizar Variables de Entorno

### 4.1 Actualizar .env

```bash
# === PUERTOS PUBLICADOS ===
# ... (existentes) ...
EMBEDDINGS_PORT=8090  # NUEVO: Puerto para servicio de embeddings

# === CONFIGURACIÃ“N RAG ===
EMBEDDING_SERVICE_HOST=embeddings-api
EMBEDDING_SERVICE_PORT=8080  # Puerto interno del contenedor
EMBEDDING_DIMENSION=768      # DimensiÃ³n Nomic

# LLM para generaciÃ³n (separado de embeddings)
RAG_LLM_HOST=mistral-7b
RAG_LLM_PORT=8080
```

**Status**: â³ Pendiente

---

## ğŸ“‹ FASE 5: Limpiar y Recrear Base Vectorial

### 5.1 Borrar volumen de Milvus (CRÃTICO)

```bash
# Detener servicios
docker compose down

# Borrar volumen de Milvus (cambia dimensiÃ³n de 4096 â†’ 768)
docker volume rm aipl_milvus_data

# Reiniciar
docker compose up -d
```

**RazÃ³n**: Milvus no puede cambiar dimensiones en colecciones existentes. Debe recrearse limpia.

**Status**: â³ Pendiente

### 5.2 Actualizar milvus_database.py

El cÃ³digo ya tiene auto-fix de dimensiÃ³n en `_create_collections()`:

```python
# Verificar dimensiÃ³n existente
if utility.has_collection(chunks_collection_name):
    collection = Collection(chunks_collection_name)
    schema = collection.schema
    
    # Buscar campo embedding
    for field in schema.fields:
        if field.name == "embedding":
            existing_dim = field.params.get('dim')
            if existing_dim != config.EMBEDDING_DIMENSION:
                logger.warning(f"âš ï¸ DimensiÃ³n incorrecta: {existing_dim} != {config.EMBEDDING_DIMENSION}")
                logger.info("ğŸ”„ Recreando colecciÃ³n con dimensiÃ³n correcta...")
                utility.drop_collection(chunks_collection_name)
                recreate_collection = True
```

**Status**: âœ… Ya implementado (auto-recreaciÃ³n)

---

## ğŸ“‹ FASE 6: Testing Completo

### 6.1 Test de Embeddings API

```bash
# Verificar que Nomic estÃ¡ respondiendo
curl -X POST http://localhost:8090/v1/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "input": "Â¿CuÃ¡ndo vence el contrato?",
    "model": "nomic-embed-text-v1.5"
  }'

# Debe retornar:
# {
#   "data": [
#     {
#       "embedding": [0.123, -0.456, ...],  # 768 valores
#       "index": 0
#     }
#   ],
#   "model": "nomic-embed-text-v1.5"
# }
```

### 6.2 Test de Upload

```bash
# Subir documento de prueba
curl -X POST http://localhost:8004/upload \
  -F "file=@test.pdf"

# Verificar logs del RAG API:
# - âœ… Debe usar embeddings-api:8080
# - âœ… DimensiÃ³n debe ser 768
# - âœ… Debe guardar en Milvus correctamente
```

### 6.3 Test de Query

```bash
# Consultar documento
curl -X POST http://localhost:8004/query \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Â¿CuÃ¡ndo vence el contrato?",
    "top_k": 5
  }'

# Verificar:
# - âœ… Embedding de query se genera con Nomic
# - âœ… BÃºsqueda en Milvus funciona
# - âœ… Respuesta se genera con Mistral/modelo seleccionado
```

**Status**: â³ Pendiente

---

## ğŸ“Š Checklist de ImplementaciÃ³n

### Fase 1: PreparaciÃ³n
- [ ] Actualizar docker-compose.yaml con descarga de Nomic
- [ ] Crear servicio embeddings-api
- [ ] Agregar EMBEDDINGS_PORT a .env

### Fase 2: ConfiguraciÃ³n
- [ ] Actualizar rag/config.py (dimensiÃ³n 768)
- [ ] Actualizar rag/embeddings.py (endpoint correcto)
- [ ] Verificar separaciÃ³n LLM vs Embeddings

### Fase 3: Limpieza
- [ ] docker compose down
- [ ] docker volume rm aipl_milvus_data
- [ ] docker compose up -d

### Fase 4: VerificaciÃ³n
- [ ] Test embeddings-api health
- [ ] Test upload documento
- [ ] Test query con RAG
- [ ] Verificar logs (sin errores)

### Fase 5: DocumentaciÃ³n
- [ ] Actualizar README.md
- [ ] Documentar arquitectura final
- [ ] Agregar troubleshooting

---

## ğŸ¯ Resultado Esperado

### Antes (âŒ MALO)
```
Upload PDF â†’ Gemma-2B (embeddings 2048) â†’ Milvus
Query â†’ Gemma-2B (embeddings 2048) â†’ Milvus â†’ Gemma-2B (respuesta)
```
**Problemas**: Lento, calidad baja, dimensiÃ³n excesiva

### DespuÃ©s (âœ… BUENO)
```
Upload PDF â†’ Nomic (embeddings 768) â†’ Milvus
Query â†’ Nomic (embeddings 768) â†’ Milvus â†’ Mistral (respuesta)
```
**Beneficios**: 
- âš¡ 5x mÃ¡s rÃ¡pido
- ğŸ¯ Mejor precisiÃ³n en bÃºsqueda
- ğŸ’¾ Menor uso de memoria
- ğŸ”§ EspecializaciÃ³n correcta

---

## ğŸ“ Notas Importantes

1. **NO mezclar roles**: Nomic SOLO embeddings, Mistral SOLO generaciÃ³n
2. **DimensiÃ³n fija**: Siempre 768 para Nomic
3. **Limpiar Milvus**: Obligatorio al cambiar dimensiÃ³n
4. **Healthcheck**: Esperar a que embeddings-api estÃ© listo antes de subir docs

---

**Fecha**: 2025-11-27
**Status**: ğŸ”´ EN PROGRESO
**Responsable**: GitHub Copilot
