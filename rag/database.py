"""
Gestor de base de datos RAG - MODO B√ÅSICO (sin pgvector)
Compatible con PostgreSQL en PowerPC sin extensiones ML
"""
import os
import logging
from typing import List, Dict, Tuple, Optional
import psycopg2
from psycopg2.extras import RealDictCursor
from sqlalchemy import create_engine, text
from sqlalchemy.pool import NullPool

from config import config

logger = logging.getLogger(__name__)

class RAGDatabase:
    """Gestor de base de datos para RAG (modo b√°sico sin pgvector)"""
    
    def __init__(self):
        """Inicializar conexi√≥n a PostgreSQL"""
        # ‚úÖ FIX: Usar el m√©todo get_database_url() en lugar del atributo
        self.engine = create_engine(
            config.get_database_url(),
            poolclass=NullPool,
            echo=False
        )
        
        # Inicializar esquema
        self._initialize_schema()
    
    def _initialize_schema(self):
        """Inicializar esquema con detecci√≥n autom√°tica de pgvector"""
        with self.engine.connect() as conn:
            try:
                # Intentar crear extensi√≥n pgvector
                pgvector_available = False
                try:
                    conn.execute(text("CREATE EXTENSION IF NOT EXISTS vector"))
                    conn.commit()
                    pgvector_available = True
                    logger.info("‚úÖ Extensi√≥n pgvector habilitada")
                except Exception as e:
                    # ‚úÖ FIX: Hacer rollback expl√≠cito para limpiar estado de transacci√≥n
                    conn.rollback()
                    logger.warning(f"‚ö†Ô∏è pgvector no disponible: {e}")
                    logger.info("üìù Usando modo b√°sico sin embeddings vectoriales")
                
                # ‚úÖ Tabla de documentos
                conn.execute(text("""
                    CREATE TABLE IF NOT EXISTS documents (
                        id SERIAL PRIMARY KEY,
                        filename VARCHAR(500) NOT NULL,
                        content_type VARCHAR(100),
                        file_size INTEGER,
                        total_chunks INTEGER DEFAULT 0,
                        metadata JSONB DEFAULT '{}'::jsonb,
                        uploaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """))
                
                # ‚úÖ Tabla de chunks (con o sin embedding seg√∫n disponibilidad)
                if pgvector_available:
                    conn.execute(text(f"""
                        CREATE TABLE IF NOT EXISTS document_chunks (
                            id SERIAL PRIMARY KEY,
                            document_id INTEGER REFERENCES documents(id) ON DELETE CASCADE,
                            chunk_index INTEGER NOT NULL,
                            content TEXT NOT NULL,
                            embedding vector({config.EMBEDDING_DIMENSION}),
                            metadata JSONB DEFAULT '{{}}'::jsonb,
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                        )
                    """))
                else:
                    conn.execute(text("""
                        CREATE TABLE IF NOT EXISTS document_chunks (
                            id SERIAL PRIMARY KEY,
                            document_id INTEGER REFERENCES documents(id) ON DELETE CASCADE,
                            chunk_index INTEGER NOT NULL,
                            content TEXT NOT NULL,
                            metadata JSONB DEFAULT '{}'::jsonb,
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                        )
                    """))
                
                # ‚úÖ √çndices
                conn.execute(text("""
                    CREATE INDEX IF NOT EXISTS idx_chunks_document 
                    ON document_chunks(document_id)
                """))
                
                if pgvector_available:
                    # √çndice HNSW para b√∫squeda vectorial r√°pida
                    try:
                        conn.execute(text("""
                            CREATE INDEX IF NOT EXISTS idx_chunks_embedding 
                            ON document_chunks USING hnsw (embedding vector_cosine_ops)
                        """))
                    except:
                        logger.warning("‚ö†Ô∏è No se pudo crear √≠ndice HNSW, usando tabla sin √≠ndice vectorial")
                
                # √çndice para b√∫squeda de texto completo (siempre √∫til)
                conn.execute(text("""
                    CREATE INDEX IF NOT EXISTS idx_chunks_content 
                    ON document_chunks USING gin(to_tsvector('spanish', content))
                """))
                
                conn.commit()
                
                if pgvector_available:
                    logger.info(f"‚úÖ Esquema RAG con pgvector inicializado (dim: {config.EMBEDDING_DIMENSION})")
                else:
                    logger.info("‚úÖ Esquema RAG inicializado en modo b√°sico (sin embeddings)")
                    # Deshabilitar embeddings en config
                    config.ENABLE_EMBEDDINGS = False
                
            except Exception as e:
                logger.error(f"‚ùå Error inicializando esquema: {e}")
                conn.rollback()  # ‚úÖ Rollback en caso de error general
                raise
    
    def insert_document(
        self,
        filename: str,
        content_type: str,
        file_size: int,
        metadata: Optional[Dict] = None
    ) -> int:
        """Insertar documento en la base de datos"""
        import json
        
        with self.engine.connect() as conn:
            result = conn.execute(
                text("""
                    INSERT INTO documents (filename, content_type, file_size, metadata)
                    VALUES (:filename, :content_type, :file_size, CAST(:metadata AS jsonb))
                    RETURNING id
                """),
                {
                    "filename": filename,
                    "content_type": content_type,
                    "file_size": file_size,
                    "metadata": json.dumps(metadata or {})
                }
            )
            conn.commit()
            doc_id = result.fetchone()[0]
            logger.info(f"‚úÖ Documento {doc_id} insertado: {filename}")
            return doc_id
    
    def insert_chunks(
        self,
        document_id: int,
        chunks: List[Tuple[int, str, List[float], Dict]]
    ):
        """Insertar chunks (con embeddings si est√°n disponibles)"""
        import json
        
        with self.engine.connect() as conn:
            for chunk_index, content, embedding, metadata in chunks:
                # Detectar si hay embeddings
                if embedding and len(embedding) > 0 and config.ENABLE_EMBEDDINGS:
                    # Insertar CON embedding vectorial
                    conn.execute(
                        text("""
                            INSERT INTO document_chunks 
                            (document_id, chunk_index, content, embedding, metadata)
                            VALUES (:doc_id, :idx, :content, :embedding::vector, CAST(:metadata AS jsonb))
                        """),
                        {
                            "doc_id": document_id,
                            "idx": chunk_index,
                            "content": content,
                            "embedding": str(embedding),  # pgvector acepta string "[1.0, 2.0, ...]"
                            "metadata": json.dumps(metadata or {})
                        }
                    )
                else:
                    # Insertar SIN embedding (modo b√°sico)
                    conn.execute(
                        text("""
                            INSERT INTO document_chunks 
                            (document_id, chunk_index, content, metadata)
                            VALUES (:doc_id, :idx, :content, CAST(:metadata AS jsonb))
                        """),
                        {
                            "doc_id": document_id,
                            "idx": chunk_index,
                            "content": content,
                            "metadata": json.dumps(metadata or {})
                        }
                    )
            
            # Actualizar contador de chunks
            conn.execute(
                text("UPDATE documents SET total_chunks = :count WHERE id = :id"),
                {"count": len(chunks), "id": document_id}
            )
            conn.commit()
            
            if config.ENABLE_EMBEDDINGS and chunks and chunks[0][2]:
                logger.info(f"‚úÖ {len(chunks)} chunks con embeddings insertados para documento {document_id}")
            else:
                logger.info(f"‚úÖ {len(chunks)} chunks insertados para documento {document_id} (sin embeddings)")
    
    def similarity_search(
        self,
        query_embedding: List[float],
        top_k: int = 5
    ) -> List[Dict]:
        """
        B√∫squeda vectorial usando pgvector (cosine similarity)
        Retorna los chunks m√°s similares sem√°nticamente
        """
        with self.engine.connect() as conn:
            results = conn.execute(
                text("""
                    SELECT 
                        dc.id,
                        dc.document_id,
                        dc.chunk_index,
                        dc.content,
                        dc.metadata,
                        d.filename,
                        1 - (dc.embedding <=> :query_emb::vector) as similarity
                    FROM document_chunks dc
                    JOIN documents d ON d.id = dc.document_id
                    WHERE dc.embedding IS NOT NULL
                    ORDER BY dc.embedding <=> :query_emb::vector
                    LIMIT :limit
                """),
                {"query_emb": str(query_embedding), "limit": top_k}
            )
            
            return [dict(row._mapping) for row in results]
    
    def text_search(self, query: str, top_k: int = 5) -> List[Dict]:
        """
        B√∫squeda de texto simple con ILIKE (compatible con cualquier PostgreSQL)
        Fallback seguro sin dependencias de diccionarios o extensiones
        """
        with self.engine.connect() as conn:
            # B√∫squeda simple con ILIKE (case-insensitive)
            # Dividir query en palabras y buscar cada una
            search_term = f"%{query}%"
            
            results = conn.execute(
                text("""
                    SELECT 
                        dc.id,
                        dc.document_id,
                        dc.chunk_index,
                        dc.content,
                        dc.metadata,
                        d.filename,
                        1.0 as rank
                    FROM document_chunks dc
                    JOIN documents d ON d.id = dc.document_id
                    WHERE dc.content ILIKE :search
                    ORDER BY dc.document_id DESC, dc.chunk_index ASC
                    LIMIT :limit
                """),
                {"search": search_term, "limit": top_k}
            )
            
            return [dict(row._mapping) for row in results]
    
    def get_all_documents(self) -> List[Dict]:
        """Obtener todos los documentos"""
        with self.engine.connect() as conn:
            results = conn.execute(
                text("""
                    SELECT id, filename, content_type, file_size, 
                           total_chunks, metadata, uploaded_at, updated_at
                    FROM documents
                    ORDER BY uploaded_at DESC
                """)
            )
            return [dict(row._mapping) for row in results]
    
    def get_document(self, document_id: int) -> Optional[Dict]:
        """Obtener documento espec√≠fico"""
        with self.engine.connect() as conn:
            result = conn.execute(
                text("""
                    SELECT id, filename, content_type, file_size,
                           total_chunks, metadata, uploaded_at, updated_at
                    FROM documents
                    WHERE id = :id
                """),
                {"id": document_id}
            )
            row = result.fetchone()
            return dict(row._mapping) if row else None
    
    def get_document_chunks(self, document_id: int) -> List[Dict]:
        """Obtener todos los chunks de un documento"""
        with self.engine.connect() as conn:
            results = conn.execute(
                text("""
                    SELECT id, document_id, chunk_index, content, metadata, created_at
                    FROM document_chunks
                    WHERE document_id = :doc_id
                    ORDER BY chunk_index
                """),
                {"doc_id": document_id}
            )
            return [dict(row._mapping) for row in results]
    
    def delete_document(self, document_id: int) -> bool:
        """Eliminar documento y sus chunks (CASCADE autom√°tico)"""
        with self.engine.connect() as conn:
            result = conn.execute(
                text("DELETE FROM documents WHERE id = :id"),
                {"id": document_id}
            )
            conn.commit()
            deleted = result.rowcount > 0
            
            if deleted:
                logger.info(f"‚úÖ Documento {document_id} eliminado")
            else:
                logger.warning(f"‚ö†Ô∏è Documento {document_id} no encontrado")
            
            return deleted
    
    def get_document_stats(self) -> Dict:
        """Obtener estad√≠sticas del sistema"""
        with self.engine.connect() as conn:
            result = conn.execute(text("""
                SELECT 
                    COUNT(DISTINCT d.id) as total_documents,
                    COUNT(dc.id) as total_chunks,
                    COALESCE(SUM(d.file_size), 0) as total_size_bytes
                FROM documents d
                LEFT JOIN document_chunks dc ON d.id = dc.document_id
            """))
            
            stats = dict(result.fetchone()._mapping)
            return stats
