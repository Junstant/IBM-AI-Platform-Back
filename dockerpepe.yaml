services:
  # === BASE DE DATOS PERSONALIZADA ===
  postgres:
    build:
      context: ./database
      dockerfile: Dockerfile
    container_name: postgres_ai_platform
    restart: always
    ports:
      - "${DB_PORT:-8070}:5432"
    environment:
      - POSTGRES_USER=${DB_USER:-postgres}
      - POSTGRES_PASSWORD=${DB_PASSWORD:-root}
      - POSTGRES_DB=postgres
      - PGDATA=/var/lib/postgresql/data
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - postgres_backups:/backups
      - ./database/init-scripts:/docker-entrypoint-initdb.d
    networks:
      - ai_platform_network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER:-postgres} -d postgres"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  # === MODELOS LLM ===
  # Modelos con descarga automÃ¡tica desde Hugging Face
  mistral-td-server:
    image: quay.io/daniel_casali/llama.cpp-mma:v5
    container_name: mistral-td-server
    restart: always
    ports:
      - "${MISTRAL_PORT:-8096}:8080"
    volumes:
      - ./models:/models
    networks:
      - ai_platform_network
    entrypoint: ["/bin/sh", "-c"]
    command: |
      "if [ ! -f /models/mistral-7b-instruct-v0.3.Q4_K_M.gguf ]; then
        echo 'ðŸ“¥ Descargando modelo Mistral 7B...'
        curl -L -o /models/mistral-7b-instruct-v0.3.Q4_K_M.gguf \
          'https://huggingface.co/SanctumAI/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/mistral-7b-instruct-v0.3.Q4_K_M.gguf'
        echo 'âœ… Modelo Mistral descargado'
      else
        echo 'âœ… Modelo Mistral ya existe'
      fi
      exec /usr/local/bin/llama-server --host 0.0.0.0 --port 8080 \
        -m /models/mistral-7b-instruct-v0.3.Q4_K_M.gguf \
        --prio 3 -c 4096 -b 32 -t 32 -n -1"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 300s

  gemma2b-td-server:
    image: quay.io/daniel_casali/llama.cpp-mma:v5
    container_name: gemma2b-td-server
    restart: always
    ports:
      - "${GEMMA_2B_PORT:-9470}:8080"
    volumes:
      - ./models:/models
    networks:
      - ai_platform_network
    entrypoint: ["/bin/sh", "-c"]
    command: |
      "if [ ! -f /models/gemma-2-2b-it-Q4_K_S.gguf ]; then
        echo 'ðŸ“¥ Descargando modelo Gemma 2B...'
        curl -L -o /models/gemma-2-2b-it-Q4_K_S.gguf \
          'https://huggingface.co/lmstudio-community/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q4_K_S.gguf'
        echo 'âœ… Modelo Gemma 2B descargado'
      else
        echo 'âœ… Modelo Gemma 2B ya existe'
      fi
      exec /usr/local/bin/llama-server --host 0.0.0.0 --port 8080 \
        -m /models/gemma-2-2b-it-Q4_K_S.gguf \
        --prio 3 -c 2048 -b 32 -t 16 -n -1"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 300s

  granite-td-server:
    image: quay.io/daniel_casali/llama.cpp-mma:v5
    container_name: granite-td-server
    restart: always
    ports:
      - "${GRANITE_PORT:-8095}:8080"
    volumes:
      - ./models:/models
    networks:
      - ai_platform_network
    entrypoint: ["/bin/sh", "-c"]
    command: |
      "if [ ! -f /models/granite-3.3-8b-instruct-Q4_K_M.gguf ]; then
        echo 'ðŸ“¥ Descargando modelo Granite 3.3 8B...'
        curl -L -o /models/granite-3.3-8b-instruct-Q4_K_M.gguf \
          'https://huggingface.co/bartowski/granite-3.3-8b-instruct-GGUF/resolve/main/granite-3.3-8b-instruct-Q4_K_M.gguf'
        echo 'âœ… Modelo Granite descargado'
      else
        echo 'âœ… Modelo Granite ya existe'
      fi
      exec /usr/local/bin/llama-server --host 0.0.0.0 --port 8080 \
        -m /models/granite-3.3-8b-instruct-Q4_K_M.gguf \
        --prio 3 -c 4096 -b 32 -t 32 -n -1"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 300s

  # Nuevos modelos con descarga automÃ¡tica
  google_gemma12b-td-server:
    image: quay.io/daniel_casali/llama.cpp-mma:v5
    container_name: google_gemma12b-td-server
    restart: always
    ports:
      - "${GEMMA_12B_PORT:-2005}:8080"
    volumes:
      - ./models:/models
    networks:
      - ai_platform_network
    entrypoint: ["/bin/sh", "-c"]
    command: |
      "if [ ! -f /models/google_gemma-3-12b-it-Q4_K_M.gguf ]; then
        echo 'ðŸ“¥ Descargando modelo Gemma 3 12B...'
        curl -L -o /models/google_gemma-3-12b-it-Q4_K_M.gguf \
          'https://huggingface.co/bartowski/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q4_K_M.gguf'
        echo 'âœ… Modelo Gemma 12B descargado'
      else
        echo 'âœ… Modelo Gemma 12B ya existe'
      fi
      exec /usr/local/bin/llama-server --host 0.0.0.0 --port 8080 \
        -m /models/google_gemma-3-12b-it-Q4_K_M.gguf --prio 3 -c 8192 -b 32 -t 32 -n -1"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 600s

  google_gemma4b-td-server:
    image: quay.io/daniel_casali/llama.cpp-mma:v5
    container_name: google_gemma4b-td-server
    restart: always
    ports:
      - "${GEMMA_4B_PORT:-8094}:8080"
    volumes:
      - ./models:/models
    networks:
      - ai_platform_network
    entrypoint: ["/bin/sh", "-c"]
    command: |
      "if [ ! -f /models/google_gemma-3-4b-it-IQ4_NL.gguf ]; then
        echo 'ðŸ“¥ Descargando modelo Gemma 3 4B...'
        curl -L -o /models/google_gemma-3-4b-it-IQ4_NL.gguf \
          'https://huggingface.co/bartowski/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-IQ4_NL.gguf'
        echo 'âœ… Modelo Gemma 4B descargado'
      else
        echo 'âœ… Modelo Gemma 4B ya existe'
      fi
      exec /usr/local/bin/llama-server --host 0.0.0.0 --port 8080 \
        -m /models/google_gemma-3-4b-it-IQ4_NL.gguf --prio 3 -c 4096 -b 32 -t 32 -n -1"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 300s

  deepseek8b-td-server:
    image: quay.io/daniel_casali/llama.cpp-mma:v5
    container_name: deepseek8b-td-server
    restart: always
    ports:
      - "${DEEPSEEK_8B_PORT:-8092}:8080"
    volumes:
      - ./models:/models
    networks:
      - ai_platform_network
    entrypoint: ["/bin/sh", "-c"]
    command: |
      "if [ ! -f /models/DeepSeek-R1-Distill-Llama-8B-Q4_K_L.gguf ]; then
        echo 'ðŸ“¥ Descargando modelo DeepSeek 8B...'
        curl -L -o /models/DeepSeek-R1-Distill-Llama-8B-Q4_K_L.gguf \
          'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q4_K_L.gguf'
        echo 'âœ… Modelo DeepSeek 8B descargado'
      else
        echo 'âœ… Modelo DeepSeek 8B ya existe'
      fi
      exec /usr/local/bin/llama-server --host 0.0.0.0 --port 8080 \
        -m /models/DeepSeek-R1-Distill-Llama-8B-Q4_K_L.gguf --prio 3 -c 4096 -b 32 -t 32 -n -1"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 300s

  deepseek1.5B-td-server:
    image: quay.io/daniel_casali/llama.cpp-mma:v5
    container_name: deepseek1.5B-td-server
    restart: always
    ports:
      - "${DEEPSEEK_1_5B_PORT:-8091}:8080"
    volumes:
      - ./models:/models
    networks:
      - ai_platform_network
    entrypoint: ["/bin/sh", "-c"]
    command: |
      "if [ ! -f /models/DeepSeek-R1-Distill-Qwen-1.5B-Q8_0.gguf ]; then
        echo 'ðŸ“¥ Descargando modelo DeepSeek 1.5B...'
        curl -L -o /models/DeepSeek-R1-Distill-Qwen-1.5B-Q8_0.gguf \
          'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q8_0.gguf'
        echo 'âœ… Modelo DeepSeek 1.5B descargado'
      else
        echo 'âœ… Modelo DeepSeek 1.5B ya existe'
      fi
      exec /usr/local/bin/llama-server --host 0.0.0.0 --port 8080 \
        -m /models/DeepSeek-R1-Distill-Qwen-1.5B-Q8_0.gguf --prio 3 -c 2048 -b 32 -t 16 -n -1"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 200s

  deepseek14B-td-server:
    image: quay.io/daniel_casali/llama.cpp-mma:v5
    container_name: deepseek14B-td-server
    restart: always
    ports:
      - "${DEEPSEEK_14B_PORT:-8090}:8080"
    volumes:
      - ./models:/models
    networks:
      - ai_platform_network
    entrypoint: ["/bin/sh", "-c"]
    command: |
      "if [ ! -f /models/DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf ]; then
        echo 'ðŸ“¥ Descargando modelo DeepSeek 14B...'
        curl -L -o /models/DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf \
          'https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf'
        echo 'âœ… Modelo DeepSeek 14B descargado'
      else
        echo 'âœ… Modelo DeepSeek 14B ya existe'
      fi
      exec /usr/local/bin/llama-server --host 0.0.0.0 --port 8080 \
        -m /models/DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf --prio 3 -c 8192 -b 32 -t 32 -n -1"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 600s

  granite-2b-server:
    image: quay.io/daniel_casali/llama.cpp-mma:v5
    container_name: granite-2b-server
    restart: always
    ports:
      - "${GRANITE_2B_PORT:-8097}:8080"
    volumes:
      - ./models:/models
    networks:
      - ai_platform_network
    entrypoint: ["/bin/sh", "-c"]
    command: |
      "if [ ! -f /models/granite-3.1-2b-instruct-Q4_K_M.gguf ]; then
        echo 'ðŸ“¥ Descargando modelo Granite 2B...'
        curl -L -o /models/granite-3.1-2b-instruct-Q4_K_M.gguf \
          'https://huggingface.co/bartowski/granite-3.1-2b-instruct-GGUF/resolve/main/granite-3.1-2b-instruct-Q4_K_M.gguf'
        echo 'âœ… Modelo Granite 2B descargado'
      else
        echo 'âœ… Modelo Granite 2B ya existe'
      fi
      exec /usr/local/bin/llama-server --host 0.0.0.0 --port 8080 \
        -m /models/granite-3.1-2b-instruct-Q4_K_M.gguf --prio 3 -c 2048 -b 32 -t 16 -n -1"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 200s

  gpt-oss-20b-server:
    image: quay.io/daniel_casali/llama.cpp-mma:v5
    container_name: gpt-oss-20b-server
    restart: always
    ports:
      - "${GPT_OSS_20B_PORT:-8098}:8080"
    volumes:
      - ./models:/models
    networks:
      - ai_platform_network
    entrypoint: ["/bin/sh", "-c"]
    command: |
      "if [ ! -f /models/gpt-oss-20b-f16.gguf ]; then
        echo 'ðŸ“¥ Descargando modelo GPT OSS 20B...'
        curl -L -o /models/gpt-oss-20b-f16.gguf \
          'https://huggingface.co/mradermacher/GPT-OSS-20B-GGUF/resolve/main/GPT-OSS-20B.f16.gguf'
        echo 'âœ… Modelo GPT OSS 20B descargado'
      else
        echo 'âœ… Modelo GPT OSS 20B ya existe'
      fi
      exec /usr/local/bin/llama-server --host 0.0.0.0 --port 8080 \
        -m /models/gpt-oss-20b-f16.gguf --prio 3 -c 4096 -b 32 -t 32 -n -1"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 900s

  # === APLICACIONES ===
  textosql-api:
    build: 
      context: ./textoSql
      dockerfile: Dockerfile
    container_name: textosql-api
    restart: always
    ports:
      - "${TEXTOSQL_API_PORT:-8001}:8000"
    env_file:
      - .env
    environment:
      - DB_NAME_TEXTOSQL=${DB_NAME_TEXTOSQL:-banco_global}
      - DB_HOST=postgres_ai_platform
      - DB_PORT=5432
      - DB_USER=${DB_USER:-postgres}
      - DB_PASSWORD=${DB_PASSWORD:-root}
      - LLM_HOST=${LLM_HOST:-localhost}
      - GRANITE_PORT=${GRANITE_PORT:-8095}
    volumes:
      - ./textoSql:/app
    networks:
      - ai_platform_network
    depends_on:
      postgres:
        condition: service_healthy
      granite-td-server:
        condition: service_started
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  fraude-api:
    build: 
      context: ./fraude
      dockerfile: Dockerfile
    container_name: fraude-api
    restart: always
    ports:
      - "${FRAUDE_API_PORT:-8000}:8000"
    env_file:
      - .env
    environment:
      - DB_NAME_FRAUDE=${DB_NAME_FRAUDE:-bank_transactions}
      - DB_HOST=postgres_ai_platform
      - DB_PORT=5432
      - DB_USER=${DB_USER:-postgres}
      - DB_PASSWORD=${DB_PASSWORD:-root}
    volumes:
      - ./fraude:/app
    networks:
      - ai_platform_network
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/"]
      interval: 30s
      timeout: 10s
      retries: 3


volumes:
  postgres_data:
    driver: local
  postgres_backups:
    driver: local

networks:
  ai_platform_network:
    driver: bridge
    name: ${COMPOSE_PROJECT_NAME:-platform_ai_lj}_network