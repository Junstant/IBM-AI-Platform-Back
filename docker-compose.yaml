services:
  #? ======================== POSTGRES ========================
  postgres:
    build:
      context: ./database
      dockerfile: Dockerfile
    image: postgres-pgvector:17-alpine
    container_name: postgres_db
    restart: always
    ports:
      - "${DB_PORT:-8070}:5432"
    environment:
      - POSTGRES_USER=${DB_USER:-postgres}
      - POSTGRES_PASSWORD=${DB_PASSWORD:-postgres}
      - POSTGRES_DB=postgres
      - PGDATA=/var/lib/postgresql/data
    volumes:
      - aipl_postgres_data:/var/lib/postgresql/data
      - ./database/init-scripts:/docker-entrypoint-initdb.d
    networks:
      - ai_platform_network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER:-postgres} -d postgres"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 90s  # AUMENTADO de 60s a 90s (scripts de inicializaci√≥n tardan)

#? ======================== FRONTEND (NGINX) ========================
  frontend:
    container_name: frontend
    restart: always
    build:
      context: ../FrontAI
      dockerfile: Dockerfile
      args:
        - VITE_API_HOST=${VITE_API_HOST}
        - VITE_GEMMA_2B_PORT=${GEMMA_2B_PORT}  
        - VITE_GEMMA_4B_PORT=${GEMMA_4B_PORT}     
        - VITE_GEMMA_12B_PORT=${GEMMA_12B_PORT}   
        - VITE_MISTRAL_PORT=${MISTRAL_PORT}        
        - VITE_DEEPSEEK_8B_PORT=${DEEPSEEK_8B_PORT}
        - VITE_FRAUD_DETECTION_PORT=${FRAUDE_API_PORT}
        - VITE_TEXTOSQL_API_PORT=${TEXTOSQL_API_PORT}
        - VITE_STATS_API_PORT=${STATS_PORT}
        - VITE_RAG_API_PORT=${RAG_API_PORT}
        - VITE_EMBEDDINGS_PORT=${EMBEDDINGS_PORT}
    ports:
      - "${NGINX_PORT:-2012}:80"
    networks:
      - ai_platform_network
    environment:
      - NGINX_PORT_INTERNAL=${NGINX_INTERNAL_PORT:-80}
      - FRAUDE_API_HOST=${FRAUDE_API_HOST:-fraude-api}
      - FRAUDE_API_PORT=${API_INTERNAL_PORT:-8000}
      - TEXTOSQL_API_HOST=${TEXTOSQL_API_HOST:-textosql-api}
      - TEXTOSQL_API_PORT=${API_INTERNAL_PORT:-8000}
      - STATS_API_HOST=${STATS_API_HOST:-stats-api}
      - STATS_API_PORT=${STATS_PORT:-8003}
      - RAG_API_HOST=${RAG_API_HOST:-rag-api}
      - RAG_API_PORT=${RAG_API_PORT:-8004}
      - GEMMA_2B_PORT=${GEMMA_2B_PORT:-8085}
      - GEMMA_4B_PORT=${GEMMA_4B_PORT:-8086}
      - GEMMA_12B_PORT=${GEMMA_12B_PORT:-8087}
      - MISTRAL_PORT=${MISTRAL_PORT:-8088}
      - DEEPSEEK_8B_PORT=${DEEPSEEK_8B_PORT:-8089}
    depends_on:
      # CAMBIO: Solo verificar que los servicios est√©n INICIADOS, no HEALTHY
      # Nginx puede arrancar aunque los backends no est√©n listos (mostrar√° errores 502 temporales)
      fraude-api:
        condition: service_started  # ‚úÖ Cambiado de service_healthy
        restart: true
      textosql-api:
        condition: service_started  # ‚úÖ Cambiado de service_healthy
        restart: true
      stats-api:
        condition: service_started  # ‚úÖ Cambiado de service_healthy
        restart: true
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  #? ======================== MODEL DOWNLOADER ========================
  model-downloader:
    image: alpine:latest
    container_name: model-downloader
    user: "root"
    volumes:
      - models_volume:/models
    environment:
      - TOKEN_HUGGHINGFACE=${TOKEN_HUGGHINGFACE}
    command: >
      sh -c '
        # Instalar wget
        apk add --no-cache wget

        # Gemma 2B
        GEMMA_PATH="/models/gemma-2-2b-it-Q4_K_S.gguf";
        if [ ! -f "$$GEMMA_PATH" ]; then
          echo "Modelo Gemma 2B no encontrado. Descargando...";
          wget --header="Authorization: Bearer $$TOKEN_HUGGHINGFACE" "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q4_K_S.gguf" -O "$$GEMMA_PATH";
          echo "‚úÖ Modelo Gemma 2B descargado.";
        else
          echo "‚úÖ Modelo Gemma 2B ya existe.";
        fi

        # Gemma 4B
        GEMMA_PATH="/models/google_gemma-3-4b-it-Q4_K_S.gguf";
        if [ ! -f "$$GEMMA_PATH" ]; then
          echo "Modelo Gemma 4B no encontrado. Descargando...";
          wget --header="Authorization: Bearer $$TOKEN_HUGGHINGFACE" "https://huggingface.co/bartowski/google_gemma-3-4b-it-GGUF/resolve/main/google_gemma-3-4b-it-Q4_K_S.gguf" -O "$$GEMMA_PATH";
          echo "‚úÖ Modelo Gemma 4B descargado.";
        else
          echo "‚úÖ Modelo Gemma 4B ya existe.";
        fi

        # Gemma 12B
        GEMMA_PATH="/models/google_gemma-3-12b-it-Q4_K_S.gguf";
        if [ ! -f "$$GEMMA_PATH" ]; then
          echo "Modelo Gemma 12B no encontrado. Descargando...";
          wget --header="Authorization: Bearer $$TOKEN_HUGGHINGFACE" "https://huggingface.co/bartowski/google_gemma-3-12b-it-GGUF/resolve/main/google_gemma-3-12b-it-Q4_K_S.gguf" -O "$$GEMMA_PATH";
          echo "‚úÖ Modelo Gemma 12B descargado.";
        else
          echo "‚úÖ Modelo Gemma 12B ya existe.";
        fi

        # Mistral 7B
        MISTRAL_PATH="/models/Mistral-7B-Instruct-v0.3-Q4_K_S.gguf";
        if [ ! -f "$$MISTRAL_PATH" ]; then
          echo "Modelo Mistral 7B no encontrado. Descargando...";
          wget --header="Authorization: Bearer $$TOKEN_HUGGHINGFACE" "https://huggingface.co/bartowski/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3-Q4_K_S.gguf" -O "$$MISTRAL_PATH";
          echo "‚úÖ Modelo Mistral 7B descargado.";
        else
          echo "‚úÖ Modelo Mistral 7B ya existe.";
        fi

        # Deepseek 8B
        DEEPSEEK_PATH="/models/DeepSeek-R1-Distill-Qwen-7B-Q4_K_L.gguf";
        if [ ! -f "$$DEEPSEEK_PATH" ]; then
          echo "Modelo Deepseek 8B no encontrado. Descargando...";
          wget --header="Authorization: Bearer $$TOKEN_HUGGHINGFACE" "https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Q4_K_L.gguf" -O "$$DEEPSEEK_PATH";
          echo "‚úÖ Modelo Deepseek 8B descargado.";
        else
          echo "‚úÖ Modelo Deepseek 8B ya existe.";
        fi

        # Nomic Embed Text v1.5 - Modelo especializado en embeddings (274 MB)
        NOMIC_PATH="/models/nomic-embed-text-v1.5.Q4_K_M.gguf";
        if [ ! -f "$$NOMIC_PATH" ]; then
          echo "Modelo Nomic Embed Text v1.5 no encontrado. Descargando...";
          wget --header="Authorization: Bearer $$TOKEN_HUGGHINGFACE" "https://huggingface.co/nomic-ai/nomic-embed-text-v1.5-GGUF/resolve/main/nomic-embed-text-v1.5.Q4_K_M.gguf" -O "$$NOMIC_PATH";
          echo "‚úÖ Modelo Nomic Embed descargado (274 MB - Especializado en embeddings).";
        else
          echo "‚úÖ Modelo Nomic Embed ya existe.";
        fi
      '
    networks:
      - ai_platform_network

  #^ ======================== GEMMA 2B (PRIORIDAD ALTA) ========================
  gemma-2b:
    image: quay.io/daniel_casali/llama.cpp-mma:v8
    container_name: gemma-2b
    user: "root"
    restart: always
    ports:
      - "${GEMMA_2B_PORT:-8085}:8080"
    volumes:
      - models_volume:/models
    networks:
      - ai_platform_network
    command: [
      "--host", "0.0.0.0",
      "--port", "8080",
      "-m", "/models/gemma-2-2b-it-Q4_K_S.gguf",
      "-c", "4096",
      "-b", "512",              # Batch size aumentado para mejor throughput
      "-t", "8",
      "-n", "-1",
      "--ctx-size", "4096",
      "--parallel", "4",        # üöÄ CRITICAL: Soportar 4 requests concurrentes
      "--cont-batching",        # üöÄ Continuous batching para menor latencia
      "-fa",                    # Flash Attention (m√°s r√°pido)
      "--mlock"                 # üöÄ Lock model en RAM (evita swap, carga instant√°nea)
    ]
    depends_on:
      model-downloader:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 60s       # Menos frecuente
      timeout: 10s        
      retries: 3          # Menos reintentos
      start_period: 300s  # Reducido a 5 minutos
    deploy:
      resources:
        limits:
          cpus: '8.0'     # M√°s CPUs = carga m√°s r√°pida
          memory: 8G

  # #^ ======================== GEMMA 4B ========================
  gemma-4b:
    image: quay.io/daniel_casali/llama.cpp-mma:v8
    container_name: gemma-4b
    user: "root"
    restart: always
    ports:
      - "${GEMMA_4B_PORT:-8086}:8080"
    volumes:
      - models_volume:/models
    networks:
      - ai_platform_network
    command: [
      "--host", "0.0.0.0",
      "--port", "8080",
      "-m", "/models/google_gemma-3-4b-it-Q4_K_S.gguf",
      "-c", "4096",
      "-b", "512",
      "-t", "12",
      "-n", "-1",
      "--ctx-size", "4096",
      "--parallel", "4",
      "--cont-batching",
      "-fa",
      "--mlock"
    ]
    depends_on:
      model-downloader:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 120s
      timeout: 120s
      retries: 3
      start_period: 600s
    deploy:
      resources:
        limits:
          cpus: '12.0'
          memory: 8G

  #^ ======================== GEMMA 12B ========================
  gemma-12b:
    image: quay.io/daniel_casali/llama.cpp-mma:v8
    container_name: gemma-12b
    user: "root"
    restart: always
    ports:
      - "${GEMMA_12B_PORT:-8087}:8080"
    volumes:
      - models_volume:/models
    networks:
      - ai_platform_network
    command: [
      "--host", "0.0.0.0",
      "--port", "8080",
      "-m", "/models/google_gemma-3-12b-it-Q4_K_S.gguf",
      "-c", "4096",
      "-b", "512",
      "-t", "16",
      "-n", "-1",
      "--ctx-size", "4096",
      "--parallel", "4",
      "--cont-batching",
      "-fa",
      "--mlock"
    ]
    depends_on:
      model-downloader:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 120s
      timeout: 120s
      retries: 3
      start_period: 600s
    deploy:
      resources:
        limits:
          cpus: '16.0'
          memory: 12G

  #^ ======================== MISTRAL 7b ========================
  mistral-7b:
    image: quay.io/daniel_casali/llama.cpp-mma:v8
    container_name: mistral-7b
    user: "root"
    restart: always
    ports:
      - "${MISTRAL_PORT:-8088}:8080"
    volumes:
      - models_volume:/models
    networks:
      - ai_platform_network
    command: [
      "--host", "0.0.0.0",
      "--port", "8080",
      "-m", "/models/Mistral-7B-Instruct-v0.3-Q4_K_S.gguf",
      "-c", "4096",
      "-b", "512",
      "-t", "12",
      "-n", "-1",
      "--ctx-size", "4096",
      "--parallel", "4",
      "--cont-batching",
      "-fa",
      "--mlock"
    ]
    depends_on:
      model-downloader:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 120s
      timeout: 120s
      retries: 3
      start_period: 600s
    deploy:
      resources:
        limits:
          cpus: '12.0'
          memory: 8G

  #^ ======================== DEEPSEEK 8B ========================
  deepseek-8b:
    image: quay.io/daniel_casali/llama.cpp-mma:v8
    container_name: deepseek-8b
    user: "root"
    restart: always
    ports:
      - "${DEEPSEEK_8B_PORT:-8089}:8080"
    volumes:
      - models_volume:/models
    networks:
      - ai_platform_network
    command: [
      "--host", "0.0.0.0",
      "--port", "8080",
      "-m", "/models/DeepSeek-R1-Distill-Qwen-7B-Q4_K_L.gguf",
      "-c", "4096",
      "-b", "512",
      "-t", "14",
      "-n", "-1",
      "--ctx-size", "4096",
      "--parallel", "4",
      "--cont-batching",
      "-fa",
      "--mlock"
    ]
    depends_on:
      model-downloader:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 120s
      timeout: 120s
      retries: 3
      start_period: 600s
    deploy:
      resources:
        limits:
          cpus: '14.0'
          memory: 10G

  #^ ======================== EMBEDDINGS API (NOMIC) ========================
  embeddings-api:
    image: quay.io/daniel_casali/llama.cpp-mma:v8
    container_name: embeddings-api
    user: "root"
    restart: always
    ports:
      - "${EMBEDDINGS_PORT:-8090}:8080"
    volumes:
      - models_volume:/models
    networks:
      - ai_platform_network
    # COMANDO CR√çTICO: --embedding y --pooling mean para generar embeddings
    command: [
      "--host", "0.0.0.0",
      "--port", "8080",
      "-m", "/models/nomic-embed-text-v1.5.Q4_K_M.gguf",
      "--embedding",           # Modo embedding
      "--pooling", "mean",     # Pooling estrategia
      "-c", "8192",            # Contexto aumentado para docs grandes
      "-b", "8192",            # Batch size aumentado (fix "input is too large")
      "-t", "4",               # Threads (usar 4 de 12 cores disponibles)
      "--parallel", "4",       # üöÄ Requests concurrentes
      "--mlock"                # üöÄ Lock en RAM para velocidad
    ]
    depends_on:
      model-downloader:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    deploy:
      resources:
        limits:
          cpus: '4.0'    # Aumentado para procesamiento de docs grandes
          memory: 3G     # Aumentado para batch size 8192

  #% ======================== TEXTOSQL API ========================
  textosql-api:
    build:
      context: .
      dockerfile: ./textoSql/Dockerfile
    container_name: textosql-api
    restart: always
    ports:
      - "${TEXTOSQL_API_PORT:-8000}:8000"
    env_file:
      - .env
    environment:
      - DB_HOST=postgres_db
      - DB_PORT=5432
      - DB_USER=${DB_USER:-postgres}
      - DB_PASSWORD=${DB_PASSWORD:-root}
      - DB_NAME=${DB1_NAME:-banco_global}
      - LLM_HOST=gemma-2b
      - LLM_PORT=8080
      - STATS_API_URL=http://stats-api:8003
    volumes:
      - ./textoSql:/app
    networks:
      - ai_platform_network
    depends_on:
      postgres:
        condition: service_healthy
      gemma-2b:
        condition: service_started
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 20s
      timeout: 10s
      retries: 5
      start_period: 330s  # 5.5 min (espera a gemma-2b)

  #% ======================== FRAUDE API ========================
  fraude-api:
    build:
      context: .
      dockerfile: ./fraude/Dockerfile
    container_name: fraude-api
    restart: always
    ports:
      - "${FRAUDE_API_PORT:-8001}:8000"
    env_file:
      - .env
    environment:
      - DB_HOST=postgres_db
      - DB_PORT=5432
      - DB_USER=${DB_USER:-postgres}
      - DB_PASSWORD=${DB_PASSWORD:-root}
      - DB_NAME=${DB2_NAME:-bank_transactions}
      - STATS_API_URL=http://stats-api:8003
    volumes:
      - ./fraude:/app
    networks:
      - ai_platform_network
    depends_on:
      postgres:
        condition: service_healthy
        restart: true
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 90s   # Solo 1.5 min (no depende de LLMs)

  #% ======================== STATS API ========================
  stats-api:
    build:
      context: ./stats
      dockerfile: Dockerfile
    container_name: stats-api
    restart: always
    ports:
      - "${STATS_PORT:-8003}:8003"
    env_file:
      - .env
    environment:
      - DOCKER_ENV=true
      - DB_HOST=postgres
      - DB_PORT=5432
      - DB_USER=${DB_USER:-postgres}
      - DB_PASSWORD=${DB_PASSWORD:-postgres}
      - DB_NAME=ai_platform_stats
      - DATABASE_URL=postgresql://${DB_USER:-postgres}:${DB_PASSWORD:-postgres}@postgres:5432/ai_platform_stats
      # URLs de modelos para health checks
      - GEMMA_2B_URL=http://gemma-2b:8080
      - GEMMA_4B_URL=http://gemma-4b:8080
      - GEMMA_12B_URL=http://gemma-12b:8080
      - MISTRAL_7B_URL=http://mistral-7b:8080
      - DEEPSEEK_8B_URL=http://deepseek-8b:8080
      - FRAUD_API_URL=http://fraude-api:8000
      - TEXTOSQL_API_URL=http://textosql-api:8000
    volumes:
      - ./stats:/app
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - ai_platform_network
    depends_on:
      postgres:
        condition: service_healthy
      # SOLO dependemos de postgres, NO de otros servicios
      # El health checker interno ya maneja la verificaci√≥n de modelos
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8003/health"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 120s  # 2 minutos en lugar de 1

  #% ======================== MILVUS DEPENDENCIES ========================
  
  # etcd - Metadata storage for Milvus
  etcd:
    image: quay.io/coreos/etcd:v3.5.5
    container_name: etcd
    restart: always
    environment:
      - ETCD_AUTO_COMPACTION_MODE=revision
      - ETCD_AUTO_COMPACTION_RETENTION=1000
      - ETCD_QUOTA_BACKEND_BYTES=4294967296
      - ETCD_SNAPSHOT_COUNT=50000
    volumes:
      - etcd_data:/etcd
    command: etcd -advertise-client-urls=http://127.0.0.1:2379 -listen-client-urls http://0.0.0.0:2379 --data-dir /etcd
    networks:
      - ai_platform_network
    healthcheck:
      test: ["CMD", "etcdctl", "endpoint", "health"]
      interval: 30s
      timeout: 10s
      retries: 3
  
  # MinIO - Object storage for Milvus
  minio:
    image: quay.io/minio/minio:RELEASE.2023-04-13T03-08-07Z
    container_name: minio
    restart: always
    ports:
      - "9001:9001"  # Console web
      - "9000:9000"  # API
    environment:
      MINIO_ACCESS_KEY: minioadmin
      MINIO_SECRET_KEY: minioadmin
    volumes:
      - minio_data:/data
    command: minio server /data --console-address ":9001"
    networks:
      - ai_platform_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
  
  # Milvus - Vector Database
  milvus:
    image: quay.io/mgiessing/milvus:v2.3.1  # Imagen para ppc64le
    container_name: milvus
    restart: always
    ports:
      - "${MILVUS_PORT:-19530}:19530"  # gRPC
      - "9091:9091"                    # Metrics
    environment:
      ETCD_ENDPOINTS: etcd:2379
      MINIO_ADDRESS: minio:9000
      MINIO_ACCESS_KEY: minioadmin
      MINIO_SECRET_KEY: minioadmin
    volumes:
      - milvus_data:/var/lib/milvus
    command: ["milvus", "run", "standalone"]
    networks:
      - ai_platform_network
    depends_on:
      etcd:
        condition: service_healthy
      minio:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9091/healthz"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 4G
        reservations:
          cpus: '2'
          memory: 2G

  #% ======================== RAG API (Milvus Vector Database) ========================
  rag-api:
    build:
      context: .
      dockerfile: ./rag/Dockerfile
    container_name: rag-api
    restart: always
    ports:
      - "${RAG_API_PORT:-8004}:8004"
    env_file:
      - .env
    environment:
      - DOCKER_ENV=true
      # Milvus Configuration
      - MILVUS_HOST=milvus
      - MILVUS_PORT=19530
      # LLM Configuration (SOLO para generaci√≥n de respuestas)
      - LLM_HOST=mistral-7b
      - LLM_PORT=8080
      # Embeddings Configuration (DEDICADO - Nomic)
      - EMBEDDING_SERVICE_HOST=embeddings-api
      - EMBEDDING_SERVICE_PORT=8080
      - EMBEDDING_MODEL=nomic-embed-text-v1.5
      - EMBEDDING_DIMENSION=768
      - ENABLE_EMBEDDINGS=true
      - STATS_API_URL=http://stats-api:8003
    volumes:
      - ./rag:/app
      - rag_documents:/app/documents
    networks:
      - ai_platform_network
    depends_on:
      milvus:
        condition: service_healthy
        restart: true
      embeddings-api:
        condition: service_healthy
        restart: true
      mistral-7b:
        condition: service_healthy
        restart: true
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8004/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 90s
    deploy:
      resources:
        limits:
          cpus: '6'
          memory: 6G
        reservations:
          cpus: '3'
          memory: 3G

volumes:
  aipl_postgres_data:
  rag_documents:
  models_volume:
  etcd_data:
  minio_data:
  milvus_data:

networks:
  ai_platform_network:
    driver: bridge
    name: ${COMPOSE_PROJECT_NAME:-platform_ai_lj}_network