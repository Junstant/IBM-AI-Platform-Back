services:
  #? ======================== POSTGRES ========================
  postgres:
    image: postgres:15-alpine
    container_name: postgres_ai_platform
    restart: always
    ports:
      - "${DB_PORT:-8070}:5432"
    environment:
      - POSTGRES_USER=${DB_USER:-postgres}
      - POSTGRES_PASSWORD=${DB_PASSWORD:-postgres}
      - POSTGRES_DB=postgres
      - PGDATA=/var/lib/postgresql/data
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - postgres_backups:/backups
      - ./database/init-scripts:/docker-entrypoint-initdb.d
    networks:
      - ai_platform_network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER:-postgres} -d postgres"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  # === DESCARGA DE MODELOS (Actualizado para 2 modelos) ===
  model-downloader:
    image: curlimages/curl:latest
    container_name: model-downloader
    user: "root"
    volumes:
      - models_volume:/models
    # CAMBIO: El script ahora verifica y descarga ambos modelos.
    command: >
      sh -c '
        # Modelo 1: Gemma
        GEMMA_PATH="/models/google_gemma-3-4b-it-Q4_K_M.gguf";
        if [ ! -f "$$GEMMA_PATH" ]; then
          echo "Modelo Gemma no encontrado. Descargando...";
          curl -L "https://huggingface.co/bartowski/google_gemma-3-4b-it-GGUF/resolve/main/google_gemma-3-4b-it-Q4_K_M.gguf" --output "$$GEMMA_PATH";
          echo "✅ Modelo Gemma descargado.";
        else
          echo "✅ Modelo Gemma ya existe.";
        fi

        # Modelo 2: Mistral
        MISTRAL_PATH="/models/mistral-7b-instruct-v0.3.Q4_K_M.gguf";
        if [ ! -f "$$MISTRAL_PATH" ]; then
          echo "Modelo Mistral no encontrado. Descargando...";
          curl -L "https://huggingface.co/lmstudio-ai/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3-Q4_K_M.gguf" --output "$$MISTRAL_PATH";
          echo "✅ Modelo Mistral descargado.";
        else
          echo "✅ Modelo Mistral ya existe.";
        fi
      '
    networks:
      - ai_platform_network

  #^ ======================== GEMMA 4B ========================
  gemma-4b:
    image: quay.io/daniel_casali/llama.cpp-mma:v8
    container_name: gemma-4b
    user: "root"
    restart: always
    ports:
      - "${LLM_GEMMA_PORT:-8080}:8080"
    volumes:
      - models_volume:/models
    networks:
      - ai_platform_network
    command: [
      "--host", "0.0.0.0",
      "--port", "8080",
      "-m", "/models/google_gemma-3-4b-it-Q4_K_M.gguf",
      "--prio", "3",
      "-c", "4096",
      "-b", "32",
      "-t", "32",
      "-n", "-1"
    ]
    depends_on:
      model-downloader:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 180s

  #^ ======================== MISTRAL 7b ========================
  mistral-7b:
    image: quay.io/daniel_casali/llama.cpp-mma:v8
    container_name: mistral-7b
    user: "root"
    restart: always
    # Expone el modelo en un puerto diferente (8081) para evitar conflictos.
    ports:
      - "${LLM_MISTRAL_PORT:-8081}:8080"
    volumes:
      - models_volume:/models
    networks:
      - ai_platform_network
    # El comando ahora apunta al archivo del modelo Mistral.
    command: [
      "--host", "0.0.0.0",
      "--port", "8080",
      "-m", "/models/mistral-7b-instruct-v0.3.Q4_K_M.gguf",
      "--prio", "3",
      "-c", "4096",
      "-b", "32",
      "-t", "32",
      "-n", "-1"
    ]
    depends_on:
      model-downloader:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 180s

  #% ======================== TEXTOSQL API ========================
  textosql-api:
    build: 
      context: ./textoSql
      dockerfile: Dockerfile
    container_name: textosql-api
    restart: always
    ports:
      - "${TEXTOSQL_API_PORT:-8001}:8000"
    env_file:
      - .env
    environment:
      - DB_NAME_TEXTOSQL=${DB_NAME_TEXTOSQL:-banco_global}
      - DB_HOST=postgres_ai_platform
      - DB_PORT=5432
      - DB_USER=${DB_USER:-postgres}
      - DB_PASSWORD=${DB_PASSWORD:-postgres}
      - LLM_HOST=gemma-4b
      - LLM_PORT=8080
    volumes:
      - ./textoSql:/app
    networks:
      - ai_platform_network
    depends_on:
      postgres:
        condition: service_healthy
      gemma-4b:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 120s

  #% ======================== FRAUDE API ========================
  fraude-api:
    build: 
      context: ./fraude
      dockerfile: Dockerfile
    container_name: fraude-api
    restart: always
    ports:
      - "${FRAUDE_API_PORT:-8000}:8000"
    env_file:
      - .env
    environment:
      - DB_HOST=postgres_ai_platform
      - DB_PORT=5432
      - DB_USER=${DB_USER:-postgres}
      - DB_PASSWORD=${DB_PASSWORD:-postgres}
    volumes:
      - ./fraude:/app
    networks:
      - ai_platform_network
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 120s

volumes:
  postgres_data:
  postgres_backups:
  models_volume:

networks:
  ai_platform_network:
    driver: bridge
    name: ${COMPOSE_PROJECT_NAME:-platform_ai_lj}_network

