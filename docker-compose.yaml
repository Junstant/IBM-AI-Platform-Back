services:
  #? ======================== POSTGRES ========================
  postgres:
    image: postgres:15-alpine
    container_name: postgres_db
    restart: always
    ports:
      - "${DB_PORT:-8070}:5432"
    environment:
      - POSTGRES_USER=${DB_USER:-postgres}
      - POSTGRES_PASSWORD=${DB_PASSWORD:-postgres}
      - POSTGRES_DB=postgres
      - PGDATA=/var/lib/postgresql/data
    volumes:
      - ./database/init-scripts:/docker-entrypoint-initdb.d
    networks:
      - ai_platform_network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER:-postgres} -d postgres"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

#? ======================== FRONTEND (NGINX) ========================
  frontend:
    container_name: frontend
    restart: always
    build:
      context: ../FrontAI
      dockerfile: Dockerfile
      args:
        # Esto pasa las variables al momento de construir el JS
        - VITE_API_HOST=${VITE_API_HOST}
        - VITE_GEMMA_2B_PORT=${GEMMA_2B_PORT}  
        - VITE_GEMMA_4B_PORT=${GEMMA_4B_PORT}     
        - VITE_GEMMA_12B_PORT=${GEMMA_12B_PORT}   
        - VITE_MISTRAL_PORT=${MISTRAL_PORT}        
        - VITE_DEEPSEEK_8B_PORT=${DEEPSEEK_8B_PORT}
        - VITE_DEEPSEEK_14B_PORT=${DEEPSEEK_14B_PORT} 
        - VITE_FRAUD_DETECTION_PORT=${FRAUDE_API_PORT}
        - VITE_TEXTOSQL_API_PORT=${TEXTOSQL_API_PORT}
        - VITE_STATS_API_PORT=${STATS_PORT}
    ports:
      - "${NGINX_PORT:-2012}:80"
    networks:
      - ai_platform_network
    environment:
      # Esto pasa las variables para que Nginx genere su config al arrancar
      - NGINX_PORT_INTERNAL=80
      - FRAUDE_API_HOST=fraude-api
      - FRAUDE_API_PORT=8000
      - TEXTOSQL_API_HOST=textosql-api
      - TEXTOSQL_API_PORT=8000
      - STATS_API_HOST=stats-api
      - STATS_API_PORT=8003
      - GEMMA_2B_PORT=${GEMMA_2B_PORT}
      - GEMMA_4B_PORT=${GEMMA_4B_PORT}
      - GEMMA_12B_PORT=${GEMMA_12B_PORT}
      - MISTRAL_PORT=${MISTRAL_PORT}
      - DEEPSEEK_8B_PORT=${DEEPSEEK_8B_PORT}
      - DEEPSEEK_14B_PORT=${DEEPSEEK_14B_PORT}
    depends_on:
      fraude-api:
        condition: service_healthy
      textosql-api:
        condition: service_healthy
      stats-api:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost"] # localhost dentro del contenedor es el puerto 80
      interval: 300s
      timeout: 300s
      retries: 3
      start_period: 120s

  #? ======================== MODEL DOWNLOADER ========================
  model-downloader:
    image: alpine:latest
    container_name: model-downloader
    user: "root"
    volumes:
      - models_volume:/models
    environment:
      - TOKEN_HUGGHINGFACE=${TOKEN_HUGGHINGFACE}
    command: >
      sh -c '
        # Instalar wget
        apk add --no-cache wget

        # Gemma 2B
        GEMMA_PATH="/models/gemma-2-2b-it-Q4_K_S.gguf";
        if [ ! -f "$$GEMMA_PATH" ]; then
          echo "Modelo Gemma 2B no encontrado. Descargando...";
          wget --header="Authorization: Bearer $$TOKEN_HUGGHINGFACE" "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q4_K_S.gguf" -O "$$GEMMA_PATH";
          echo "✅ Modelo Gemma 2B descargado.";
        else
          echo "✅ Modelo Gemma 2B ya existe.";
        fi

        # Gemma 4B
        GEMMA_PATH="/models/google_gemma-3-4b-it-Q4_K_S.gguf";
        if [ ! -f "$$GEMMA_PATH" ]; then
          echo "Modelo Gemma 4B no encontrado. Descargando...";
          wget --header="Authorization: Bearer $$TOKEN_HUGGHINGFACE" "https://huggingface.co/bartowski/google_gemma-3-4b-it-GGUF/resolve/main/google_gemma-3-4b-it-Q4_K_S.gguf" -O "$$GEMMA_PATH";
          echo "✅ Modelo Gemma 4B descargado.";
        else
          echo "✅ Modelo Gemma 4B ya existe.";
        fi

        # Gemma 12B
        GEMMA_PATH="/models/google_gemma-3-12b-it-Q4_K_S.gguf";
        if [ ! -f "$$GEMMA_PATH" ]; then
          echo "Modelo Gemma 12B no encontrado. Descargando...";
          wget --header="Authorization: Bearer $$TOKEN_HUGGHINGFACE" "https://huggingface.co/bartowski/google_gemma-3-12b-it-GGUF/resolve/main/google_gemma-3-12b-it-Q4_K_S.gguf" -O "$$GEMMA_PATH";
          echo "✅ Modelo Gemma 12B descargado.";
        else
          echo "✅ Modelo Gemma 12B ya existe.";
        fi

        # Mistral 7B
        MISTRAL_PATH="/models/Mistral-7B-Instruct-v0.3-Q4_K_S.gguf";
        if [ ! -f "$$MISTRAL_PATH" ]; then
          echo "Modelo Mistral 7B no encontrado. Descargando...";
          wget --header="Authorization: Bearer $$TOKEN_HUGGHINGFACE" "https://huggingface.co/bartowski/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3-Q4_K_S.gguf" -O "$$MISTRAL_PATH";
          echo "✅ Modelo Mistral 7B descargado.";
        else
          echo "✅ Modelo Mistral 7B ya existe.";
        fi

        # Deepseek 8B
        DEEPSEEK_PATH="/models/DeepSeek-R1-Distill-Qwen-7B-Q4_K_L.gguf";
        if [ ! -f "$$DEEPSEEK_PATH" ]; then
          echo "Modelo Deepseek 8B no encontrado. Descargando...";
          wget --header="Authorization: Bearer $$TOKEN_HUGGHINGFACE" "https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Q4_K_L.gguf" -O "$$DEEPSEEK_PATH";
          echo "✅ Modelo Deepseek 8B descargado.";
        else
          echo "✅ Modelo Deepseek 8B ya existe.";
        fi

        # Deepseek 14B
        DEEPSEEK_PATH="/models/DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf";
        if [ ! -f "$$DEEPSEEK_PATH" ]; then
          echo "Modelo Deepseek 14B no encontrado. Descargando...";
          wget --header="Authorization: Bearer $$TOKEN_HUGGHINGFACE" "https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf" -O "$$DEEPSEEK_PATH";
          echo "✅ Modelo Deepseek 14B descargado.";
        else
          echo "✅ Modelo Deepseek 14B ya existe.";
        fi

      '
    networks:
      - ai_platform_network

  #^ ======================== GEMMA 2B ========================
  gemma-2b:
    image: quay.io/daniel_casali/llama.cpp-mma:v8
    container_name: gemma-2b
    user: "root"
    restart: always
    ports:
      - "${GEMMA_2B_PORT:-8085}:8080"
    volumes:
      - models_volume:/models
    networks:
      - ai_platform_network
    command: ["--host", "0.0.0.0", "--port", "8080", "-m", "/models/gemma-2-2b-it-Q4_K_S.gguf", "--prio", "3", "-c", "4096", "-b", "32", "-t", "32", "-n", "-1"]
    depends_on:
      model-downloader:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 120s
      timeout: 120s
      retries: 3
      start_period: 600s

  # #^ ======================== GEMMA 4B ========================
  # gemma-4b:
  #   image: quay.io/daniel_casali/llama.cpp-mma:v8
  #   container_name: gemma-4b
  #   user: "root"
  #   restart: always
  #   ports:
  #     - "${GEMMA_4B_PORT:-8086}:8080"
  #   volumes:
  #     - models_volume:/models
  #   networks:
  #     - ai_platform_network
  #   command: ["--host", "0.0.0.0", "--port", "8080", "-m", "/models/google_gemma-3-4b-it-Q4_K_S.gguf", "--prio", "3", "-c", "4096", "-b", "32", "-t", "32", "-n", "-1"]
  #   depends_on:
  #     model-downloader:
  #       condition: service_completed_successfully
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
  #     interval: 120s
  #     timeout: 120s
  #     retries: 3
  #     start_period: 600s

  #^ ======================== GEMMA 12B ========================
  gemma-12b:
    image: quay.io/daniel_casali/llama.cpp-mma:v8
    container_name: gemma-12b
    user: "root"
    restart: always
    ports:
      - "${GEMMA_12B_PORT:-8087}:8080"
    volumes:
      - models_volume:/models
    networks:
      - ai_platform_network
    command: ["--host", "0.0.0.0", "--port", "8080", "-m", "/models/google_gemma-3-12b-it-Q4_K_S.gguf", "--prio", "3", "-c", "4096", "-b", "32", "-t", "32", "-n", "-1"]
    depends_on:
      model-downloader:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 120s
      timeout: 120s
      retries: 3
      start_period: 600s

  #^ ======================== MISTRAL 7b ========================
  mistral-7b:
    image: quay.io/daniel_casali/llama.cpp-mma:v8
    container_name: mistral-7b
    user: "root"
    restart: always
    ports:
      - "${MISTRAL_PORT:-8088}:8080"
    volumes:
      - models_volume:/models
    networks:
      - ai_platform_network
    command: ["--host", "0.0.0.0", "--port", "8080", "-m", "/models/Mistral-7B-Instruct-v0.3-Q4_K_S.gguf", "--prio", "3", "-c", "4096", "-b", "32", "-t", "32", "-n", "-1"]
    depends_on:
      model-downloader:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 120s
      timeout: 120s
      retries: 3
      start_period: 600s
  

  #^ ======================== DEEPSEEK 8B ========================
  deepseek-8b:
    image: quay.io/daniel_casali/llama.cpp-mma:v8
    container_name: deepseek-8b
    user: "root"
    restart: always
    ports:
      - "${DEEPSEEK_8B_PORT:-8089}:8080"
    volumes:
      - models_volume:/models
    networks:
      - ai_platform_network
    command: ["--host", "0.0.0.0", "--port", "8080", "-m", "/models/DeepSeek-R1-Distill-Qwen-7B-Q4_K_L.gguf", "--prio", "3", "-c", "4096", "-b", "32", "-t", "32", "-n", "-1"]
    depends_on:
      model-downloader:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 120s
      timeout: 120s
      retries: 3
      start_period: 600s

  #^ ======================== DEEPSEEK 14B ========================
  deepseek-14b:
    image: quay.io/daniel_casali/llama.cpp-mma:v8
    container_name: deepseek-14b
    user: "root"
    restart: always
    ports:
      - "${DEEPSEEK_14B_PORT:-8090}:8080"
    volumes:
      - models_volume:/models
    networks:
      - ai_platform_network
    command: ["--host", "0.0.0.0", "--port", "8080", "-m", "/models/DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf", "--prio", "3", "-c", "4096", "-b", "32", "-t", "32", "-n", "-1"]
    depends_on:
      model-downloader:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 120s
      timeout: 120s
      retries: 3
      start_period: 600s

  #% ======================== FRAUDE API ========================
  fraude-api:
    build:
      context: ./fraude
      dockerfile: Dockerfile
    container_name: fraude-api
    restart: always
    ports:
      - "${FRAUDE_API_PORT:-8001}:8000"
    env_file:
      - .env
    environment:
      - DB_HOST=postgres_db
      - DB_PORT=5432
      - DB_USER=${DB_USER:-postgres}
      - DB_PASSWORD=${DB_PASSWORD:-root}
      - DB_NAME=${DB2_NAME:-bank_transactions}
    volumes:
      - ./fraude:/app
    networks:
      - ai_platform_network
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s  # ← Cambiar de 120s a 30s
      timeout: 10s   # ← Cambiar de 120s a 10s
      retries: 10    # ← Más reintentos
      start_period: 180s  # ← Más tiempo de espera inicial

  #% ======================== TEXTOSQL API ========================
  textosql-api:
    build:
      context: ./textoSql
      dockerfile: Dockerfile
    container_name: textosql-api
    restart: always
    ports:
      - "${TEXTOSQL_API_PORT:-8000}:8000"
    env_file:
      - .env
    environment:
      - DB_HOST=postgres_db
      - DB_PORT=5432
      - DB_USER=${DB_USER:-postgres}
      - DB_PASSWORD=${DB_PASSWORD:-root}
      - DB_NAME=${DB1_NAME:-banco_global}
      - LLM_HOST=gemma-4b
      - LLM_PORT=8080
    volumes:
      - ./textoSql:/app
    networks:
      - ai_platform_network
    depends_on:
      postgres:
        condition: service_healthy
      gemma-4b:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 120s
      timeout: 120s
      retries: 3
      start_period: 120s

  #📊 ======================== STATS API ========================
  stats-api:
    build:
      context: ./stats
      dockerfile: Dockerfile
    container_name: stats-api
    restart: always
    ports:
      - "${STATS_PORT:-8003}:8003"
    env_file:
      - .env
    environment:
      - DOCKER_ENV=true
      - DB_HOST=postgres
      - DB_PORT=5432
      - DB_USER=${DB_USER:-postgres}
      - DB_PASSWORD=${DB_PASSWORD:-postgres}
      - DB_NAME=ai_platform_stats
      - DATABASE_URL=postgresql://${DB_USER:-postgres}:${DB_PASSWORD:-postgres}@postgres:5432/ai_platform_stats
      # URLs de modelos para health checks
      - GEMMA_2B_URL=http://gemma-2b:8080
      - GEMMA_4B_URL=http://gemma-4b:8080
      - GEMMA_12B_URL=http://gemma-12b:8080
      - MISTRAL_7B_URL=http://mistral-7b:8080
      - DEEPSEEK_8B_URL=http://deepseek-8b:8080
      - DEEPSEEK_14B_URL=http://deepseek-14b:8080
      - FRAUD_API_URL=http://fraude-api:8000
      - TEXTOSQL_API_URL=http://textosql-api:8000
    volumes:
      - ./stats:/app
      - /var/run/docker.sock:/var/run/docker.sock:ro  # Para monitorear Docker
    networks:
      - ai_platform_network
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8003/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

volumes:
  models_volume:

networks:
  ai_platform_network:
    driver: bridge
    name: ${COMPOSE_PROJECT_NAME:-platform_ai_lj}_network