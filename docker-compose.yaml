services:
  #? ======================== POSTGRES ========================
  postgres:
    image: postgres:15-alpine
    container_name: postgres_db
    restart: always
    ports:
      - "${DB_PORT:-8070}:5432"
    environment:
      - POSTGRES_USER=${DB_USER:-postgres}
      - POSTGRES_PASSWORD=${DB_PASSWORD:-postgres}
      - POSTGRES_DB=postgres
      - PGDATA=/var/lib/postgresql/data
    volumes:
      - ./database/init-scripts:/docker-entrypoint-initdb.d
    networks:
      - ai_platform_network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER:-postgres} -d postgres"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 90s  # AUMENTADO de 60s a 90s (scripts de inicialización tardan)

#? ======================== FRONTEND (NGINX) ========================
  frontend:
    container_name: frontend
    restart: always
    build:
      context: ../FrontAI
      dockerfile: Dockerfile
      args:
        - VITE_API_HOST=${VITE_API_HOST}
        - VITE_GEMMA_2B_PORT=${GEMMA_2B_PORT}  
        - VITE_GEMMA_4B_PORT=${GEMMA_4B_PORT}     
        - VITE_GEMMA_12B_PORT=${GEMMA_12B_PORT}   
        - VITE_MISTRAL_PORT=${MISTRAL_PORT}        
        - VITE_DEEPSEEK_8B_PORT=${DEEPSEEK_8B_PORT}
        - VITE_FRAUD_DETECTION_PORT=${FRAUDE_API_PORT}
        - VITE_TEXTOSQL_API_PORT=${TEXTOSQL_API_PORT}
        - VITE_STATS_API_PORT=${STATS_PORT}
    ports:
      - "${NGINX_PORT:-2012}:80"
    networks:
      - ai_platform_network
    environment:
      - NGINX_PORT_INTERNAL=80
      - FRAUDE_API_HOST=fraude-api
      - FRAUDE_API_PORT=8000
      - TEXTOSQL_API_HOST=textosql-api
      - TEXTOSQL_API_PORT=8000
      - STATS_API_HOST=stats-api
      - STATS_API_PORT=8003
      - GEMMA_2B_PORT=${GEMMA_2B_PORT}
      - GEMMA_4B_PORT=${GEMMA_4B_PORT}
      - GEMMA_12B_PORT=${GEMMA_12B_PORT}
      - MISTRAL_PORT=${MISTRAL_PORT}
      - DEEPSEEK_8B_PORT=${DEEPSEEK_8B_PORT}
    depends_on:
      # CAMBIO: Solo verificar que los servicios estén INICIADOS, no HEALTHY
      # Nginx puede arrancar aunque los backends no estén listos (mostrará errores 502 temporales)
      fraude-api:
        condition: service_started  # ✅ Cambiado de service_healthy
        restart: true
      textosql-api:
        condition: service_started  # ✅ Cambiado de service_healthy
        restart: true
      stats-api:
        condition: service_started  # ✅ Cambiado de service_healthy
        restart: true
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  #? ======================== MODEL DOWNLOADER ========================
  model-downloader:
    image: alpine:latest
    container_name: model-downloader
    user: "root"
    volumes:
      - models_volume:/models
    environment:
      - TOKEN_HUGGHINGFACE=${TOKEN_HUGGHINGFACE}
    command: >
      sh -c '
        # Instalar wget
        apk add --no-cache wget

        # Gemma 2B
        GEMMA_PATH="/models/gemma-2-2b-it-Q4_K_S.gguf";
        if [ ! -f "$$GEMMA_PATH" ]; then
          echo "Modelo Gemma 2B no encontrado. Descargando...";
          wget --header="Authorization: Bearer $$TOKEN_HUGGHINGFACE" "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q4_K_S.gguf" -O "$$GEMMA_PATH";
          echo "✅ Modelo Gemma 2B descargado.";
        else
          echo "✅ Modelo Gemma 2B ya existe.";
        fi

        # Gemma 4B
        GEMMA_PATH="/models/google_gemma-3-4b-it-Q4_K_S.gguf";
        if [ ! -f "$$GEMMA_PATH" ]; then
          echo "Modelo Gemma 4B no encontrado. Descargando...";
          wget --header="Authorization: Bearer $$TOKEN_HUGGHINGFACE" "https://huggingface.co/bartowski/google_gemma-3-4b-it-GGUF/resolve/main/google_gemma-3-4b-it-Q4_K_S.gguf" -O "$$GEMMA_PATH";
          echo "✅ Modelo Gemma 4B descargado.";
        else
          echo "✅ Modelo Gemma 4B ya existe.";
        fi

        # Gemma 12B
        GEMMA_PATH="/models/google_gemma-3-12b-it-Q4_K_S.gguf";
        if [ ! -f "$$GEMMA_PATH" ]; then
          echo "Modelo Gemma 12B no encontrado. Descargando...";
          wget --header="Authorization: Bearer $$TOKEN_HUGGHINGFACE" "https://huggingface.co/bartowski/google_gemma-3-12b-it-GGUF/resolve/main/google_gemma-3-12b-it-Q4_K_S.gguf" -O "$$GEMMA_PATH";
          echo "✅ Modelo Gemma 12B descargado.";
        else
          echo "✅ Modelo Gemma 12B ya existe.";
        fi

        # Mistral 7B
        MISTRAL_PATH="/models/Mistral-7B-Instruct-v0.3-Q4_K_S.gguf";
        if [ ! -f "$$MISTRAL_PATH" ]; then
          echo "Modelo Mistral 7B no encontrado. Descargando...";
          wget --header="Authorization: Bearer $$TOKEN_HUGGHINGFACE" "https://huggingface.co/bartowski/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3-Q4_K_S.gguf" -O "$$MISTRAL_PATH";
          echo "✅ Modelo Mistral 7B descargado.";
        else
          echo "✅ Modelo Mistral 7B ya existe.";
        fi

        # Deepseek 8B
        DEEPSEEK_PATH="/models/DeepSeek-R1-Distill-Qwen-7B-Q4_K_L.gguf";
        if [ ! -f "$$DEEPSEEK_PATH" ]; then
          echo "Modelo Deepseek 8B no encontrado. Descargando...";
          wget --header="Authorization: Bearer $$TOKEN_HUGGHINGFACE" "https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Q4_K_L.gguf" -O "$$DEEPSEEK_PATH";
          echo "✅ Modelo Deepseek 8B descargado.";
        else
          echo "✅ Modelo Deepseek 8B ya existe.";
        fi
      '
    networks:
      - ai_platform_network

  #^ ======================== GEMMA 2B (PRIORIDAD ALTA) ========================
  gemma-2b:
    image: quay.io/daniel_casali/llama.cpp-mma:v8
    container_name: gemma-2b
    user: "root"
    restart: always
    ports:
      - "${GEMMA_2B_PORT:-8085}:8080"
    volumes:
      - models_volume:/models
    networks:
      - ai_platform_network
    command: ["--host", "0.0.0.0", "--port", "8080", "-m", "/models/gemma-2-2b-it-Q4_K_S.gguf", "-c", "4096", "-b", "128", "-t", "8", "-n", "-1", "--ctx-size", "4096"]
    depends_on:
      model-downloader:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 60s       # Menos frecuente
      timeout: 10s        
      retries: 3          # Menos reintentos
      start_period: 300s  # Reducido a 5 minutos
    deploy:
      resources:
        limits:
          cpus: '8.0'     # Más CPUs = carga más rápida
          memory: 8G

  # #^ ======================== GEMMA 4B ========================
  gemma-4b:
    image: quay.io/daniel_casali/llama.cpp-mma:v8
    container_name: gemma-4b
    user: "root"
    restart: always
    ports:
      - "${GEMMA_4B_PORT:-8086}:8080"
    volumes:
      - models_volume:/models
    networks:
      - ai_platform_network
    command: ["--host", "0.0.0.0", "--port", "8080", "-m", "/models/google_gemma-3-4b-it-Q4_K_S.gguf", "-c", "4096", "-b", "256", "-t", "12", "-n", "-1", "--ctx-size", "4096"]
    depends_on:
      model-downloader:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 120s
      timeout: 120s
      retries: 3
      start_period: 600s
    profiles:
      - full
    deploy:
      resources:
        limits:
          cpus: '12.0'
          memory: 8G

  #^ ======================== GEMMA 12B ========================
  gemma-12b:
    image: quay.io/daniel_casali/llama.cpp-mma:v8
    container_name: gemma-12b
    user: "root"
    restart: always
    ports:
      - "${GEMMA_12B_PORT:-8087}:8080"
    volumes:
      - models_volume:/models
    networks:
      - ai_platform_network
    command: ["--host", "0.0.0.0", "--port", "8080", "-m", "/models/google_gemma-3-12b-it-Q4_K_S.gguf", "-c", "4096", "-b", "256", "-t", "16", "-n", "-1", "--ctx-size", "4096"]
    depends_on:
      model-downloader:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 120s
      timeout: 120s
      retries: 3
      start_period: 600s
    profiles:
      - full
    deploy:
      resources:
        limits:
          cpus: '16.0'
          memory: 12G

  #^ ======================== MISTRAL 7b ========================
  mistral-7b:
    image: quay.io/daniel_casali/llama.cpp-mma:v8
    container_name: mistral-7b
    user: "root"
    restart: always
    ports:
      - "${MISTRAL_PORT:-8088}:8080"
    volumes:
      - models_volume:/models
    networks:
      - ai_platform_network
    command: ["--host", "0.0.0.0", "--port", "8080", "-m", "/models/Mistral-7B-Instruct-v0.3-Q4_K_S.gguf", "-c", "4096", "-b", "256", "-t", "12", "-n", "-1", "--ctx-size", "4096"]
    depends_on:
      model-downloader:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 120s
      timeout: 120s
      retries: 3
      start_period: 600s
    profiles:
      - full
    deploy:
      resources:
        limits:
          cpus: '12.0'
          memory: 8G

  #^ ======================== DEEPSEEK 8B ========================
  deepseek-8b:
    image: quay.io/daniel_casali/llama.cpp-mma:v8
    container_name: deepseek-8b
    user: "root"
    restart: always
    ports:
      - "${DEEPSEEK_8B_PORT:-8089}:8080"
    volumes:
      - models_volume:/models
    networks:
      - ai_platform_network
    command: ["--host", "0.0.0.0", "--port", "8080", "-m", "/models/DeepSeek-R1-Distill-Qwen-7B-Q4_K_L.gguf", "-c", "4096", "-b", "256", "-t", "14", "-n", "-1", "--ctx-size", "4096"]
    depends_on:
      model-downloader:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 120s
      timeout: 120s
      retries: 3
      start_period: 600s
    profiles:
      - full
    deploy:
      resources:
        limits:
          cpus: '14.0'
          memory: 10G

  #% ======================== TEXTOSQL API ========================
  textosql-api:
    build:
      context: ./textoSql
      dockerfile: Dockerfile
    container_name: textosql-api
    restart: always
    ports:
      - "${TEXTOSQL_API_PORT:-8000}:8000"
    env_file:
      - .env
    environment:
      - DB_HOST=postgres_db
      - DB_PORT=5432
      - DB_USER=${DB_USER:-postgres}
      - DB_PASSWORD=${DB_PASSWORD:-root}
      - DB_NAME=${DB1_NAME:-banco_global}
      - LLM_HOST=gemma-2b
      - LLM_PORT=8080
    volumes:
      - ./textoSql:/app
    networks:
      - ai_platform_network
    depends_on:
      postgres:
        condition: service_healthy
      gemma-2b:
        condition: service_started
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 20s
      timeout: 10s
      retries: 5
      start_period: 330s  # 5.5 min (espera a gemma-2b)

  #% ======================== FRAUDE API ========================
  fraude-api:
    build:
      context: ./fraude
      dockerfile: Dockerfile
    container_name: fraude-api
    restart: always
    ports:
      - "${FRAUDE_API_PORT:-8001}:8000"
    env_file:
      - .env
    environment:
      - DB_HOST=postgres_db
      - DB_PORT=5432
      - DB_USER=${DB_USER:-postgres}
      - DB_PASSWORD=${DB_PASSWORD:-root}
      - DB_NAME=${DB2_NAME:-bank_transactions}
    volumes:
      - ./fraude:/app
    networks:
      - ai_platform_network
    depends_on:
      postgres:
        condition: service_healthy
        restart: true
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 90s   # Solo 1.5 min (no depende de LLMs)

  #% ======================== STATS API ========================
  stats-api:
    build:
      context: ./stats
      dockerfile: Dockerfile
    container_name: stats-api
    restart: always
    ports:
      - "${STATS_PORT:-8003}:8003"
    env_file:
      - .env
    environment:
      - DOCKER_ENV=true
      - DB_HOST=postgres
      - DB_PORT=5432
      - DB_USER=${DB_USER:-postgres}
      - DB_PASSWORD=${DB_PASSWORD:-postgres}
      - DB_NAME=ai_platform_stats
      - DATABASE_URL=postgresql://${DB_USER:-postgres}:${DB_PASSWORD:-postgres}@postgres:5432/ai_platform_stats
      # URLs de modelos para health checks
      - GEMMA_2B_URL=http://gemma-2b:8080
      - GEMMA_4B_URL=http://gemma-4b:8080
      - GEMMA_12B_URL=http://gemma-12b:8080
      - MISTRAL_7B_URL=http://mistral-7b:8080
      - DEEPSEEK_8B_URL=http://deepseek-8b:8080
      - FRAUD_API_URL=http://fraude-api:8000
      - TEXTOSQL_API_URL=http://textosql-api:8000
    volumes:
      - ./stats:/app
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - ai_platform_network
    depends_on:
      postgres:
        condition: service_healthy
      # SOLO dependemos de postgres, NO de otros servicios
      # El health checker interno ya maneja la verificación de modelos
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8003/health"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 120s  # 2 minutos en lugar de 1

volumes:
  models_volume:

networks:
  ai_platform_network:
    driver: bridge
    name: ${COMPOSE_PROJECT_NAME:-platform_ai_lj}_network