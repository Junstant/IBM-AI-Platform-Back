version: "3.9"

services:
  # === BASE DE DATOS ===
  postgres:
    image: postgres:latest
    container_name: postgres
    restart: always
    ports:
      - "${DB_PORT}:5432"
    environment:
      - POSTGRES_USER=${DB_USER}
      - POSTGRES_PASSWORD=${DB_PASSWORD}
      - POSTGRES_DB=${DB_NAME_TEXTOSQL}
    volumes:
      - postgres_data:/var/lib/postgresql/data

  # === MODELOS LLM ===
  # Modelos existentes que coinciden con archivos
  mistral-td-server:
    image: quay.io/daniel_casali/llama.cpp-mma:v5
    container_name: mistral-td-server
    restart: always
    ports:
      - "${MISTRAL_PORT}:8080"
    volumes:
      - ./models:/models
    command: >
      /usr/local/bin/llama-server
      -m /models/mistral-7b-instruct-v0.3.Q4_K_M.gguf
      --host 0.0.0.0
      --port 8080
      -c 4096

  gemma2b-td-server:
    image: quay.io/daniel_casali/llama.cpp-mma:v5
    container_name: gemma2b-td-server
    restart: always
    ports:
      - "${GEMMA_2B_PORT}:8080"
    volumes:
      - ./models:/models
    command: >
      /usr/local/bin/llama-server
      -m /models/gemma-2-2b-it-Q4_K_S.gguf
      --host 0.0.0.0
      --port 8080
      -c 2048

  granite-td-server:
    image: quay.io/daniel_casali/llama.cpp-mma:v5
    container_name: granite-td-server
    restart: always
    ports:
      - "${GRANITE_PORT}:8080"
    volumes:
      - ./models:/models
    command: >
      /usr/local/bin/llama-server
      -m /models/granite-3.3-8b-instruct-Q4_K_M.gguf
      --host 0.0.0.0
      --port 8080
      -c 4096

  # Nuevos modelos basados en archivos disponibles
  google_gemma12b-td-server:
    image: quay.io/daniel_casali/llama.cpp-mma:v5
    container_name: google_gemma12b-td-server
    restart: always
    ports:
      - "${GEMMA_12B_PORT}:8080"
    volumes:
      - ./models:/models
    command: >
      /usr/local/bin/llama-server
      -m /models/google_gemma-3-12b-it-Q4_K_M.gguf
      --host 0.0.0.0
      --port 8080
      -c 8192

  google_gemma4b-td-server:
    image: quay.io/daniel_casali/llama.cpp-mma:v5
    container_name: google_gemma4b-td-server
    restart: always
    ports:
      - "${GEMMA_4B_PORT}:8080"
    volumes:
      - ./models:/models
    command: >
      /usr/local/bin/llama-server
      -m /models/google_gemma-3-4b-it-IQ4_NL.gguf
      --host 0.0.0.0
      --port 8080
      -c 4096

  deepseek8b-td-server:
    image: quay.io/daniel_casali/llama.cpp-mma:v5
    container_name: deepseek8b-td-server
    restart: always
    ports:
      - "${DEEPSEEK_8B_PORT}:8080"
    volumes:
      - ./models:/models
    command: >
      /usr/local/bin/llama-server
      -m /models/DeepSeek-R1-Distill-Llama-8B-Q4_K_L.gguf
      --host 0.0.0.0
      --port 8080
      -c 4096

  deepseek1.5B-td-server:
    image: quay.io/daniel_casali/llama.cpp-mma:v5
    container_name: deepseek1.5B-td-server
    restart: always
    ports:
      - "${DEEPSEEK_1_5B_PORT}:8080"
    volumes:
      - ./models:/models
    command: >
      /usr/local/bin/llama-server
      -m /models/DeepSeek-R1-Distill-Qwen-1.5B-Q8_0.gguf
      --host 0.0.0.0
      --port 8080
      -c 2048

  deepseek14B-td-server:
    image: quay.io/daniel_casali/llama.cpp-mma:v5
    container_name: deepseek14B-td-server
    restart: always
    ports:
      - "${DEEPSEEK_14B_PORT}:8080"
    volumes:
      - ./models:/models
    command: >
      /usr/local/bin/llama-server
      -m /models/DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf
      --host 0.0.0.0
      --port 8080
      -c 8192

  granite-2b-server:
    image: quay.io/daniel_casali/llama.cpp-mma:v5
    container_name: granite-2b-server
    restart: always
    ports:
      - "8097:8080"
    volumes:
      - ./models:/models
    command: >
      /usr/local/bin/llama-server
      -m /models/granite-3.1-2b-instruct-Q4_K_M.gguf
      --host 0.0.0.0
      --port 8080
      -c 2048

  gpt-oss-20b-server:
    image: quay.io/daniel_casali/llama.cpp-mma:v5
    container_name: gpt-oss-20b-server
    restart: always
    ports:
      - "8098:8080"
    volumes:
      - ./models:/models
    command: >
      /usr/local/bin/llama-server
      -m /models/gpt-oss-20b-f16.gguf
      --host 0.0.0.0
      --port 8080
      -c 4096

  # Modelos existentes que mantienen su configuración actual
  gemma-test:
    image: quay.io/daniel_casali/llama.cpp-mma:v5
    container_name: gemma-test
    restart: always
    ports:
      - "${GEMMA_TEST_PORT}:8080"
    volumes:
      - ./models:/models
    # Mantiene su configuración actual

  benja_gpt:
    image: benja_llama
    container_name: benja_gpt
    restart: always
    ports:
      - "${BENJA_GPT_PORT}:8080"
    volumes:
      - ./models:/models

  # === APLICACIONES ===
  textosql-api:
    build: ./textoSql
    container_name: textosql-api
    restart: always
    ports:
      - "${TEXTOSQL_API_PORT}:8000"
    env_file:
      - .env
    environment:
      # Asegurar que estas variables lleguen al contenedor
      - DB_NAME_TEXTOSQL=${DB_NAME_TEXTOSQL}
      - DB_HOST=${DB_HOST}
      - DB_PORT=${DB_PORT}
      - DB_USER=${DB_USER}
      - DB_PASSWORD=${DB_PASSWORD}
      - LLM_HOST=${LLM_HOST}
      - GRANITE_PORT=${GRANITE_PORT}
    volumes:
      - ./textoSql:/textoSql
    depends_on:
      - postgres
      - granite-td-server
      - gemma2b-td-server
      - mistral-td-server

  fraude-api:
    build: ./fraude
    container_name: fraude-api
    restart: always
    ports:
      - "${FRAUDE_API_PORT}:8000"
    env_file:
      - .env
    volumes:
      - ./fraude:/app
    environment:
      - DB_NAME_FRAUDE=${DB_NAME_FRAUDE}
      - DB_HOST=${DB_HOST}
      - DB_PORT=${DB_PORT}
      - DB_USER=${DB_USER}
      - DB_PASSWORD=${DB_PASSWORD}
    depends_on:
      - postgres


volumes:
  postgres_data: