services:
  #? ======================== POSTGRES ========================
  postgres:
    image: postgres:15-alpine
    container_name: postgres_db
    restart: always
    ports:
      - "${DB_PORT:-8070}:5432"
    environment:
      - POSTGRES_USER=${DB_USER:-postgres}
      - POSTGRES_PASSWORD=${DB_PASSWORD:-postgres}
      - POSTGRES_DB=postgres
      - PGDATA=/var/lib/postgresql/data
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - postgres_backups:/backups
      - ./database/init-scripts:/docker-entrypoint-initdb.d
    networks:
      - ai_platform_network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER:-postgres} -d postgres"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

#? ======================== FRONTEND (NGINX) ========================
  frontend:
    container_name: frontend
    restart: always
    # Conecta el contenedor a la red del host.
    network_mode: "host"
    build:
      context: ../FrontAI
      dockerfile: Dockerfile
      args:
        # Esto está perfecto, se mantiene igual.
        - VITE_API_HOST=${VITE_API_HOST}
        - VITE_GEMMA2b_PORT=${GEMMA_2B_PORT}
        - VITE_GEMMA4b_PORT=${GEMMA_4B_PORT}
        - VITE_GEMMA12b_PORT=${GEMMA_12B_PORT}
        - VITE_MISTRAL_PORT=${MISTRAL_PORT}
        - VITE_DEEPSEEK8b_PORT=${DEEPSEEK_8B_PORT}
        - VITE_DEEPSEEK14b_PORT=${DEEPSEEK_14B_PORT}
        - VITE_FRAUD_DETECTION_PORT=${FRAUDE_API_PORT}
        - VITE_TEXTOSQL_API_PORT=${TEXTOSQL_API_PORT}
    
    # Esta sección se ignora pero no causa error, la podés dejar.
    ports:
      - "${NGINX_PORT:-2012}:80"

    # La sección 'networks' debe estar comentada o eliminada.
    # networks:
    #   - ai_platform_network

    # --- ESTE BLOQUE ES EL QUE HAY QUE CORREGIR ---
    environment:
      # Nginx ahora debe escuchar en el puerto público que definiste en .env
      - NGINX_PORT_INTERNAL=${NGINX_PORT:-2012}
      # Las APIs se acceden a través del localhost del host y sus puertos publicados
      - FRAUDE_API_HOST=127.0.0.1
      - FRAUDE_API_PORT=${FRAUDE_API_PORT:-8001}
      - TEXTOSQL_API_HOST=127.0.0.1
      - TEXTOSQL_API_PORT=${TEXTOSQL_API_PORT:-8000}
    # -----------------------------------------------

    depends_on:
      fraude-api:
        condition: service_healthy
      textosql-api:
        condition: service_healthy
    healthcheck:
      # El healthcheck ahora debe apuntar al puerto público
      test: ["CMD", "curl", "-f", "http://localhost:${NGINX_PORT:-2012}"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 120s

  #? ======================== MODEL DOWNLOADER ========================
  model-downloader:
    image: alpine:latest
    container_name: model-downloader
    user: "root"
    volumes:
      - models_volume:/models
    environment:
      - TOKEN_HUGGHINGFACE=${TOKEN_HUGGHINGFACE}
    command: >
      sh -c '
        # Instalar wget
        apk add --no-cache wget

        # Gemma 2B
        GEMMA_PATH="/models/gemma-2-2b-it-Q4_K_S.gguf";
        if [ ! -f "$$GEMMA_PATH" ]; then
          echo "Modelo Gemma 2B no encontrado. Descargando...";
          wget --header="Authorization: Bearer $$TOKEN_HUGGHINGFACE" "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q4_K_S.gguf" -O "$$GEMMA_PATH";
          echo "✅ Modelo Gemma 2B descargado.";
        else
          echo "✅ Modelo Gemma 2B ya existe.";
        fi

        # Gemma 4B
        GEMMA_PATH="/models/google_gemma-3-4b-it-Q4_K_S.gguf";
        if [ ! -f "$$GEMMA_PATH" ]; then
          echo "Modelo Gemma 4B no encontrado. Descargando...";
          wget --header="Authorization: Bearer $$TOKEN_HUGGHINGFACE" "https://huggingface.co/bartowski/google_gemma-3-4b-it-GGUF/resolve/main/google_gemma-3-4b-it-Q4_K_S.gguf" -O "$$GEMMA_PATH";
          echo "✅ Modelo Gemma 4B descargado.";
        else
          echo "✅ Modelo Gemma 4B ya existe.";
        fi

        # Gemma 12B
        GEMMA_PATH="/models/google_gemma-3-12b-it-Q4_K_S.gguf";
        if [ ! -f "$$GEMMA_PATH" ]; then
          echo "Modelo Gemma 12B no encontrado. Descargando...";
          wget --header="Authorization: Bearer $$TOKEN_HUGGHINGFACE" "https://huggingface.co/bartowski/google_gemma-3-12b-it-GGUF/resolve/main/google_gemma-3-12b-it-Q4_K_S.gguf" -O "$$GEMMA_PATH";
          echo "✅ Modelo Gemma 12B descargado.";
        else
          echo "✅ Modelo Gemma 12B ya existe.";
        fi

        # Mistral 7B
        MISTRAL_PATH="/models/Mistral-7B-Instruct-v0.3-Q4_K_S.gguf";
        if [ ! -f "$$MISTRAL_PATH" ]; then
          echo "Modelo Mistral 7B no encontrado. Descargando...";
          wget --header="Authorization: Bearer $$TOKEN_HUGGHINGFACE" "https://huggingface.co/bartowski/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3-Q4_K_S.gguf" -O "$$MISTRAL_PATH";
          echo "✅ Modelo Mistral 7B descargado.";
        else
          echo "✅ Modelo Mistral 7B ya existe.";
        fi

        # Deepseek 8B
        DEEPSEEK_PATH="/models/DeepSeek-R1-Distill-Qwen-7B-Q4_K_L.gguf";
        if [ ! -f "$$DEEPSEEK_PATH" ]; then
          echo "Modelo Deepseek 8B no encontrado. Descargando...";
          wget --header="Authorization: Bearer $$TOKEN_HUGGHINGFACE" "https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Q4_K_L.gguf" -O "$$DEEPSEEK_PATH";
          echo "✅ Modelo Deepseek 8B descargado.";
        else
          echo "✅ Modelo Deepseek 8B ya existe.";
        fi

        # Deepseek 14B
        DEEPSEEK_PATH="/models/DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf";
        if [ ! -f "$$DEEPSEEK_PATH" ]; then
          echo "Modelo Deepseek 14B no encontrado. Descargando...";
          wget --header="Authorization: Bearer $$TOKEN_HUGGHINGFACE" "https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf" -O "$$DEEPSEEK_PATH";
          echo "✅ Modelo Deepseek 14B descargado.";
        else
          echo "✅ Modelo Deepseek 14B ya existe.";
        fi

      '
    networks:
      - ai_platform_network

  #^ ======================== GEMMA 2B ========================
  gemma-2b:
    image: quay.io/daniel_casali/llama.cpp-mma:v8
    container_name: gemma-2b
    user: "root"
    restart: always
    ports:
      - "${GEMMA_2B_PORT:-8085}:8080"
    volumes:
      - models_volume:/models
    networks:
      - ai_platform_network
    command: ["--host", "0.0.0.0", "--port", "8080", "-m", "/models/gemma-2-2b-it-Q4_K_S.gguf", "--prio", "3", "-c", "4096", "-b", "32", "-t", "32", "-n", "-1"]
    depends_on:
      model-downloader:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 90s
      timeout: 30s
      retries: 3
      start_period: 600s

  #^ ======================== GEMMA 4B ========================
  gemma-4b:
    image: quay.io/daniel_casali/llama.cpp-mma:v8
    container_name: gemma-4b
    user: "root"
    restart: always
    ports:
      - "${GEMMA_4B_PORT:-8086}:8080"
    volumes:
      - models_volume:/models
    networks:
      - ai_platform_network
    command: ["--host", "0.0.0.0", "--port", "8080", "-m", "/models/google_gemma-3-4b-it-Q4_K_S.gguf", "--prio", "3", "-c", "4096", "-b", "32", "-t", "32", "-n", "-1"]
    depends_on:
      model-downloader:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 90s
      timeout: 30s
      retries: 3
      start_period: 600s

  #^ ======================== GEMMA 12B ========================
  gemma-12b:
    image: quay.io/daniel_casali/llama.cpp-mma:v8
    container_name: gemma-12b
    user: "root"
    restart: always
    ports:
      - "${GEMMA_12B_PORT:-8087}:8080"
    volumes:
      - models_volume:/models
    networks:
      - ai_platform_network
    command: ["--host", "0.0.0.0", "--port", "8080", "-m", "/models/google_gemma-3-12b-it-Q4_K_S.gguf", "--prio", "3", "-c", "4096", "-b", "32", "-t", "32", "-n", "-1"]
    depends_on:
      model-downloader:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 90s
      timeout: 30s
      retries: 3
      start_period: 600s

  #^ ======================== MISTRAL 7b ========================
  mistral-7b:
    image: quay.io/daniel_casali/llama.cpp-mma:v8
    container_name: mistral-7b
    user: "root"
    restart: always
    ports:
      - "${MISTRAL_PORT:-8088}:8080"
    volumes:
      - models_volume:/models
    networks:
      - ai_platform_network
    command: ["--host", "0.0.0.0", "--port", "8080", "-m", "/models/Mistral-7B-Instruct-v0.3-Q4_K_S.gguf", "--prio", "3", "-c", "4096", "-b", "32", "-t", "32", "-n", "-1"]
    depends_on:
      model-downloader:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 90s
      timeout: 30s
      retries: 3
      start_period: 600s
  

  #^ ======================== DEEPSEEK 8B ========================
  deepseek-8b:
    image: quay.io/daniel_casali/llama.cpp-mma:v8
    container_name: deepseek-8b
    user: "root"
    restart: always
    ports:
      - "${DEEPSEEK_8B_PORT:-8089}:8080"
    volumes:
      - models_volume:/models
    networks:
      - ai_platform_network
    command: ["--host", "0.0.0.0", "--port", "8080", "-m", "/models/DeepSeek-R1-Distill-Qwen-7B-Q4_K_L.gguf", "--prio", "3", "-c", "4096", "-b", "32", "-t", "32", "-n", "-1"]
    depends_on:
      model-downloader:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 90s
      timeout: 30s
      retries: 3
      start_period: 600s

  #^ ======================== DEEPSEEK 14B ========================
  deepseek-14b:
    image: quay.io/daniel_casali/llama.cpp-mma:v8
    container_name: deepseek-14b
    user: "root"
    restart: always
    ports:
      - "${DEEPSEEK_14B_PORT:-8090}:8080"
    volumes:
      - models_volume:/models
    networks:
      - ai_platform_network
    command: ["--host", "0.0.0.0", "--port", "8080", "-m", "/models/DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf", "--prio", "3", "-c", "4096", "-b", "32", "-t", "32", "-n", "-1"]
    depends_on:
      model-downloader:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 90s
      timeout: 30s
      retries: 3
      start_period: 600s

  #% ======================== FRAUDE API ========================
  fraude-api:
    build:
      context: ./fraude
      dockerfile: Dockerfile
    container_name: fraude-api
    restart: always
    ports:
      - "${FRAUDE_API_PORT:-8001}:8000"
    env_file:
      - .env
    environment:
      - DB_HOST=postgres_db
      - DB_PORT=5432
      - DB_USER=${DB_USER:-postgres}
      - DB_PASSWORD=${DB_PASSWORD:-root}
      - DB_NAME=${DB2_NAME:-bank_transactions}
    volumes:
      - ./fraude:/app
    networks:
      - ai_platform_network
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 120s

  #% ======================== TEXTOSQL API ========================
  textosql-api:
    build:
      context: ./textoSql
      dockerfile: Dockerfile
    container_name: textosql-api
    restart: always
    ports:
      - "${TEXTOSQL_API_PORT:-8000}:8000"
    env_file:
      - .env
    environment:
      - DB_HOST=postgres_db
      - DB_PORT=5432
      - DB_USER=${DB_USER:-postgres}
      - DB_PASSWORD=${DB_PASSWORD:-root}
      - DB_NAME=${DB1_NAME:-banco_global}
      - LLM_HOST=gemma-4b
      - LLM_PORT=8080
    volumes:
      - ./textoSql:/app
    networks:
      - ai_platform_network
    depends_on:
      postgres:
        condition: service_healthy
      gemma-4b:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 120s

volumes:
  postgres_data:
  postgres_backups:
  models_volume:

networks:
  ai_platform_network:
    driver: bridge
    name: ${COMPOSE_PROJECT_NAME:-platform_ai_lj}_network