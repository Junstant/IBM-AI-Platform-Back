services:
  # === BASE DE DATOS ===
  postgres:
    build:
      context: ./database
      dockerfile: Dockerfile
    container_name: postgres_ai_platform
    restart: always
    ports:
      - "${DB_PORT:-8070}:5432"
    environment:
      - POSTGRES_USER=${DB_USER:-postgres}
      - POSTGRES_PASSWORD=${DB_PASSWORD:-postgres}
      - POSTGRES_DB=postgres
      - PGDATA=/var/lib/postgresql/data
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - postgres_backups:/backups
      - ./database/init-scripts:/docker-entrypoint-initdb.d
    networks:
      - ai_platform_network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER:-postgres} -d postgres"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  # === MODELOS LLM M√öLTIPLES ===
  
  # Gemma 2B - Modelo ligero y eficiente
  gemma2b-server:
    image: quay.io/daniel_casali/llama.cpp-mma:v5
    container_name: gemma2b-server
    restart: always
    ports:
      - "9470:8080"
    volumes:
      - ./models:/models
    networks:
      - ai_platform_network
    entrypoint: ["/usr/local/bin/llama-server"]
    command: [
      "--host", "0.0.0.0",
      "--port", "8080",
      "-m", "/models/gemma-2-2b-it-Q4_K_S.gguf",
      "--prio", "3",
      "-c", "2048",
      "-b", "512", 
      "-t", "16",
      "-n", "-1"
    ]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health", "||", "exit", "1"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 180s

  # Mistral 7B - Modelo principal
  mistral-server:
    image: quay.io/daniel_casali/llama.cpp-mma:v5
    container_name: mistral-server
    restart: always
    ports:
      - "8096:8080"
    volumes:
      - ./models:/models
    networks:
      - ai_platform_network
    entrypoint: ["/usr/local/bin/llama-server"]
    command: [
      "--host", "0.0.0.0", 
      "--port", "8080",
      "-m", "/models/mistral-7b-instruct-v0.3.Q4_K_M.gguf",
      "--prio", "3",
      "-c", "2048",
      "-b", "512",
      "-t", "90", 
      "-n", "-1"
    ]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health", "||", "exit", "1"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 180s

  # LLM Server principal (detecta autom√°ticamente el modelo disponible)
  llm-server:
    image: quay.io/daniel_casali/llama.cpp-mma:v5
    container_name: llm-server
    restart: always
    ports:
      - "${LLM_PORT:-8080}:8080"
    volumes:
      - ./models:/models
    networks:
      - ai_platform_network
    entrypoint: ["/bin/sh", "-c"]
    command: |
      "# Detectar modelo disponible autom√°ticamente
      MODEL_FILE=''
      if [ -f /models/gemma-2-2b-it-Q4_K_S.gguf ]; then
        MODEL_FILE='/models/gemma-2-2b-it-Q4_K_S.gguf'
        echo '‚úÖ Modelo encontrado: Gemma 2B'
      elif [ -f /models/mistral-7b-instruct-v0.3.Q4_K_M.gguf ]; then
        MODEL_FILE='/models/mistral-7b-instruct-v0.3.Q4_K_M.gguf'
        echo '‚úÖ Modelo encontrado: Mistral 7B'
      else
        echo '‚ö†Ô∏è  No se encontr√≥ ning√∫n modelo compatible'
        echo 'üìã Modelos soportados:'
        echo '  ‚Ä¢ Gemma 2B: gemma-2-2b-it-Q4_K_S.gguf'
        echo '  ‚Ä¢ Mistral 7B: mistral-7b-instruct-v0.3.Q4_K_M.gguf'
        echo ''
        echo 'üí° Use: ./ai-platform.sh para descargar modelos'
        echo 'üîÑ Reintentando en 60 segundos...'
        sleep 60
        exit 1
      fi
      
      echo 'üöÄ Iniciando servidor LLM principal con modelo: '$MODEL_FILE
      exec /usr/local/bin/llama-server --host 0.0.0.0 --port 8080 \
        --model $MODEL_FILE \
        --prio 3 -c 4096 -b 32 -t 16 -n -1"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 180s

  # === APLICACIONES ===
  textosql-api:
    build: 
      context: ./textoSql
      dockerfile: Dockerfile
    container_name: textosql-api
    restart: always
    ports:
      - "${TEXTOSQL_API_PORT:-8001}:8000"
    env_file:
      - .env
    environment:
      - DB_NAME_TEXTOSQL=${DB_NAME_TEXTOSQL:-banco_global}
      - DB_HOST=postgres_ai_platform
      - DB_PORT=5432
      - DB_USER=${DB_USER:-postgres}
      - DB_PASSWORD=${DB_PASSWORD:-postgres}
      - LLM_HOST=llm-server
      - LLM_PORT=8080
    volumes:
      - ./textoSql:/app
    networks:
      - ai_platform_network
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 120s

  fraude-api:
    build: 
      context: ./fraude
      dockerfile: Dockerfile
    container_name: fraude-api
    restart: always
    ports:
      - "${FRAUDE_API_PORT:-8000}:8000"
    env_file:
      - .env
    environment:
      - DB_HOST=postgres_ai_platform
      - DB_PORT=5432
      - DB_USER=${DB_USER:-postgres}
      - DB_PASSWORD=${DB_PASSWORD:-postgres}
    volumes:
      - ./fraude:/app
    networks:
      - ai_platform_network
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 120s

volumes:
  postgres_data:
    driver: local
  postgres_backups:
    driver: local

networks:
  ai_platform_network:
    driver: bridge
    name: ${COMPOSE_PROJECT_NAME:-platform_ai_lj}_network