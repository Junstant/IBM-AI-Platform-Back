# advanced_fraud_model.py
"""
üß† MODELO DE IA S√öPER AVANZADO PARA DETECCI√ìN DE FRAUDE
Utiliza algoritmos de Machine Learning de √∫ltima generaci√≥n.
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from sklearn.feature_selection import SelectKBest, f_classif
import joblib
import warnings
warnings.filterwarnings('ignore')

class AdvancedFraudDetector:
    """üß† Detector de fraude con IA s√∫per avanzada y algoritmos inteligentes"""
    
    def __init__(self):
        print("üß† Inicializando IA S√∫per Avanzada para Detecci√≥n de Fraude...")
        
        # M√∫ltiples modelos de IA con par√°metros m√°s sutiles
        self.models = {
            'random_forest': RandomForestClassifier(
                n_estimators=150,  # Reducido para menos overfitting
                max_depth=10,      # Reducido para predicciones m√°s sutiles
                min_samples_split=10,  # Incrementado para m√°s generalizaci√≥n
                min_samples_leaf=5,    # Incrementado para suavizar predicciones
                class_weight='balanced',
                random_state=42,
                max_features='sqrt'  # A√±adido para reducir overfitting
            ),
            'gradient_boosting': GradientBoostingClassifier(
                n_estimators=80,   # Reducido
                learning_rate=0.05, # Reducido para aprendizaje m√°s suave
                max_depth=6,       # Reducido
                min_samples_split=10,  # Incrementado
                min_samples_leaf=5,    # Incrementado
                random_state=42,
                subsample=0.8      # A√±adido para regularizaci√≥n
            )
        }
        
        self.best_model = None
        self.model_name = None
        self.train_columns = None
        self.label_encoders = {}
        self.scaler = StandardScaler()
        self.feature_selector = SelectKBest(f_classif, k=20)  # Reducido de 25 a 20
        
        # Mapas de riesgo din√°micos
        self.merchant_risk_scores = {}
        self.location_risk_scores = {}
        self.time_risk_patterns = {}
        
        # Umbrales m√°s realistas
        self.optimal_threshold = 0.4  # Reducido de 0.5 a 0.4
        self.confidence_levels = {
            'very_high': 0.85,  # Reducido de 0.9
            'high': 0.65,       # Reducido de 0.7
            'medium': 0.45,     # Reducido de 0.5
            'low': 0.25         # Reducido de 0.3
        }
        
    def _advanced_feature_engineering(self, df):
        """üß† Ingenier√≠a de caracter√≠sticas s√∫per avanzada con IA"""
        print("üîß Aplicando ingenier√≠a de caracter√≠sticas s√∫per avanzada...")
        
        # Preservar columnas originales para an√°lisis
        original_df = df.copy()
        
        # Eliminar columnas administrativas
        df = df.drop(columns=['id', 'fecha_transaccion', 'es_fraude'], errors='ignore')
        
        # === 1. PROCESAMIENTO TEMPORAL INTELIGENTE ===
        def extract_hour_from_time(time_str):
            try:
                if pd.isna(time_str):
                    return 12  # hora promedio
                if isinstance(time_str, str):
                    return int(time_str.split(':')[0])
                return int(time_str.hour) if hasattr(time_str, 'hour') else 12
            except:
                return 12
        
        df['hour'] = df['horario_transaccion'].apply(extract_hour_from_time)
        df['is_night'] = ((df['hour'] >= 23) | (df['hour'] <= 5)).astype(int)
        df['is_business_hours'] = ((df['hour'] >= 9) & (df['hour'] <= 17)).astype(int)
        df['is_very_late'] = ((df['hour'] >= 1) & (df['hour'] <= 4)).astype(int)  # M√°s espec√≠fico
        
        # === 2. AN√ÅLISIS DE MONTOS CON IA ===
        df['monto'] = pd.to_numeric(df['monto'], errors='coerce').fillna(0)
        df['monto_log'] = np.log1p(df['monto'])
        df['monto_sqrt'] = np.sqrt(df['monto'])
        
        # Categor√≠as m√°s sutiles de monto
        monto_percentiles = df['monto'].quantile([0.25, 0.5, 0.75, 0.85, 0.92, 0.97])
        df['is_small_transaction'] = (df['monto'] <= monto_percentiles[0.5]).astype(int)
        df['is_medium_transaction'] = ((df['monto'] > monto_percentiles[0.5]) & (df['monto'] <= monto_percentiles[0.85])).astype(int)
        df['is_high_transaction'] = ((df['monto'] > monto_percentiles[0.85]) & (df['monto'] <= monto_percentiles[0.97])).astype(int)
        df['is_very_high_transaction'] = (df['monto'] > monto_percentiles[0.97]).astype(int)
        
        # === 3. AN√ÅLISIS DE COMERCIANTES CON IA ===
        # Calcular scores de riesgo din√°micos con suavizado
        if 'comerciante' in original_df.columns and 'es_fraude' in original_df.columns:
            merchant_fraud_rates = original_df.groupby('comerciante')['es_fraude'].agg(['mean', 'count'])
            for merchant in merchant_fraud_rates.index:
                fraud_rate = merchant_fraud_rates.loc[merchant, 'mean']
                transaction_count = merchant_fraud_rates.loc[merchant, 'count']
                # Score suavizado con menos peso para pocos datos
                confidence = min(transaction_count / 20, 1.0)  # Incrementado de 10 a 20
                base_risk = 0.15  # Riesgo base m√°s alto
                self.merchant_risk_scores[merchant] = base_risk + (fraud_rate * confidence * 0.6)
        
        # Caracter√≠sticas de comerciantes m√°s sutiles
        df['merchant_risk_score'] = df['comerciante'].map(self.merchant_risk_scores).fillna(0.15)
        df['is_medium_risk_merchant'] = (df['merchant_risk_score'] > 0.3).astype(int)  # Umbral m√°s bajo
        df['is_high_risk_merchant'] = (df['merchant_risk_score'] > 0.5).astype(int)
        df['is_online_merchant'] = df['comerciante'].str.contains('Online|online|E-commerce', case=False, na=False).astype(int)
        df['is_financial_merchant'] = df['comerciante'].str.contains('Financiero|Financial|Western|MoneyGram|Binance', case=False, na=False).astype(int)
        
        # === 4. AN√ÅLISIS GEOGR√ÅFICO CON IA ===
        # Calcular scores de riesgo por ubicaci√≥n con suavizado
        if 'ubicacion' in original_df.columns and 'es_fraude' in original_df.columns:
            location_fraud_rates = original_df.groupby('ubicacion')['es_fraude'].agg(['mean', 'count'])
            for location in location_fraud_rates.index:
                fraud_rate = location_fraud_rates.loc[location, 'mean']
                transaction_count = location_fraud_rates.loc[location, 'count']
                confidence = min(transaction_count / 15, 1.0)  # Incrementado de 5 a 15
                base_risk = 0.12
                self.location_risk_scores[location] = base_risk + (fraud_rate * confidence * 0.5)
        
        df['location_risk_score'] = df['ubicacion'].map(self.location_risk_scores).fillna(0.12)
        df['is_medium_risk_location'] = (df['location_risk_score'] > 0.25).astype(int)
        df['is_high_risk_location'] = (df['location_risk_score'] > 0.4).astype(int)
        df['is_online_location'] = df['ubicacion'].str.contains('Online', case=False, na=False).astype(int)
        df['is_distant_location'] = (df['distancia_ubicacion_usual'] > 50).astype(int) if 'distancia_ubicacion_usual' in df.columns else 0
        
        # === 5. AN√ÅLISIS DE TARJETAS ===
        # Encoding m√°s sutil de tipos de tarjeta
        card_risk_map = {
            'D√©bito': 0.12,
            'Cr√©dito': 0.15,
            'Prepaga': 0.20,
            'Unknown': 0.25
        }
        df['card_risk_score'] = df['tipo_tarjeta'].map(card_risk_map).fillna(0.18)
        
        # === 6. CARACTER√çSTICAS COMBINADAS SUTILES ===
        df['risk_score_combined'] = (
            df['merchant_risk_score'] * 0.35 +
            df['location_risk_score'] * 0.25 +
            df['card_risk_score'] * 0.15 +
            df['is_very_late'] * 0.25
        )
        
        df['subtle_anomaly_score'] = (
            df['is_very_high_transaction'] * 0.25 +
            df['is_high_risk_merchant'] * 0.25 +
            df['is_high_risk_location'] * 0.20 +
            df['is_very_late'] * 0.15 +
            df['is_financial_merchant'] * 0.15
        )
        
        # === 7. ESTAD√çSTICAS AVANZADAS SUAVIZADAS ===
        monto_std = df['monto'].std()
        monto_mean = df['monto'].mean()
        if monto_std > 0:
            df['monto_zscore'] = np.abs((df['monto'] - monto_mean) / monto_std)
        else:
            df['monto_zscore'] = 0
        df['is_outlier_amount'] = (df['monto_zscore'] > 2.5).astype(int)  # Incrementado de 2 a 2.5
        
        # === 8. ENCODING DE VARIABLES CATEG√ìRICAS ===
        categorical_columns = ['comerciante', 'ubicacion', 'tipo_tarjeta']
        
        for col in categorical_columns:
            if col in df.columns:
                if col not in self.label_encoders:
                    self.label_encoders[col] = LabelEncoder()
                    df[f'{col}_encoded'] = self.label_encoders[col].fit_transform(df[col].astype(str))
                else:
                    # Para nuevos datos, manejar categor√≠as no vistas
                    known_categories = set(self.label_encoders[col].classes_)
                    df[col] = df[col].astype(str)
                    df[col] = df[col].apply(lambda x: x if x in known_categories else 'unknown')
                    df[f'{col}_encoded'] = self.label_encoders[col].transform(df[col])
        
        # Eliminar columnas categ√≥ricas originales y horario
        columns_to_drop = ['comerciante', 'ubicacion', 'tipo_tarjeta', 'horario_transaccion', 'cuenta_origen_id', 'cuenta_destino_id', 'distancia_ubicacion_usual']
        df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])
        
        print(f"‚úÖ Caracter√≠sticas generadas: {len(df.columns)} features sutiles")
        
        return df
    
    def prepare_data(self, transactions):
        """üß† Preparaci√≥n inteligente de datos"""
        print("üìä Preparando datos con algoritmos inteligentes...")
        
        columns = [
            'id', 'cuenta_origen_id', 'cuenta_destino_id', 'monto', 'comerciante',
            'ubicacion', 'tipo_tarjeta', 'horario_transaccion', 'fecha_transaccion', 'es_fraude'
        ]
        df = pd.DataFrame(transactions, columns=columns)
        
        print(f"üìä Dataset inicial: {len(df)} transacciones")
        
        # Informaci√≥n del dataset
        fraud_count = df['es_fraude'].sum()
        fraud_percentage = (fraud_count / len(df)) * 100
        
        print(f"üìä An√°lisis del dataset:")
        print(f"   Total transacciones: {len(df)}")
        print(f"   Fraudes: {fraud_count} ({fraud_percentage:.2f}%)")
        print(f"   Leg√≠timas: {len(df) - fraud_count} ({100 - fraud_percentage:.2f}%)")
        
        return df
    
    def train_model(self, data):
        """üß† Entrenamiento con m√∫ltiples algoritmos de IA"""
        print("\nüöÄ INICIANDO ENTRENAMIENTO CON IA S√öPER AVANZADA...")
        print("=" * 60)
        
        # Extraer target variable ANTES de feature engineering
        y = data['es_fraude'].astype(int)
        
        # ‚úÖ VERIFICAR SI HAY FRAUDES EN LOS DATOS
        fraud_count = y.sum()
        total_count = len(y)
        
        if fraud_count == 0:
            print("‚ùå ERROR CR√çTICO: No hay transacciones fraudulentas en los datos")
            print("üí° El modelo necesita ejemplos de fraude para entrenar")
            print("üîß Verifica que el script SQL 03-fraud-samples.sql se ejecute correctamente")
            return 0.0, 0.4
        
        if fraud_count < 10:
            print(f"‚ö†Ô∏è  ADVERTENCIA: Muy pocos fraudes para entrenar ({fraud_count} de {total_count})")
            print("üéØ Se recomienda al menos 50 ejemplos de fraude")
        
        print(f"üìä Balance de clases: {fraud_count} fraudes ({fraud_count/total_count*100:.1f}%) de {total_count} total")
        
        # Preparar caracter√≠sticas
        X = self._advanced_feature_engineering(data)
        
        print(f"üéØ Entrenando con {len(X)} muestras y {len(X.columns)} caracter√≠sticas")
        
        # Divisi√≥n de datos
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.25, random_state=42, stratify=y
        )
        
        # Selecci√≥n de caracter√≠sticas m√°s importantes
        X_train_selected = self.feature_selector.fit_transform(X_train, y_train)
        X_test_selected = self.feature_selector.transform(X_test)
        
        # Guardar nombres de columnas seleccionadas
        selected_features = X.columns[self.feature_selector.get_support()]
        self.train_columns = selected_features
        
        print(f"üéØ Caracter√≠sticas seleccionadas: {len(selected_features)}")
        
        # Escalado de caracter√≠sticas
        X_train_scaled = self.scaler.fit_transform(X_train_selected)
        X_test_scaled = self.scaler.transform(X_test_selected)
        
        # Entrenar m√∫ltiples modelos
        best_score = 0
        results = {}
        
        for name, model in self.models.items():
            print(f"\nüß† Entrenando modelo: {name.upper()}")
            
            # Entrenamiento
            model.fit(X_train_scaled, y_train)
            
            # Predicciones
            y_pred = model.predict(X_test_scaled)
            y_proba = model.predict_proba(X_test_scaled)[:, 1]
            
            # M√©tricas
            auc_score = roc_auc_score(y_test, y_proba)
            cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=3, scoring='roc_auc')  # Reducido de 5 a 3
            
            results[name] = {
                'model': model,
                'auc_score': auc_score,
                'cv_mean': cv_scores.mean(),
                'cv_std': cv_scores.std(),
                'predictions': y_pred,
                'probabilities': y_proba
            }
            
            print(f"   AUC Score: {auc_score:.4f}")
            print(f"   CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
            
            if auc_score > best_score:
                best_score = auc_score
                self.best_model = model
                self.model_name = name
        
        # Optimizaci√≥n del umbral
        best_proba = results[self.model_name]['probabilities']
        self.optimal_threshold = self._optimize_threshold(y_test, best_proba)
        
        # Reporte final
        print(f"\nüèÜ MEJOR MODELO: {self.model_name.upper()}")
        print(f"üéØ AUC Score: {best_score:.4f}")
        print(f"üéØ Umbral √≥ptimo: {self.optimal_threshold:.3f}")
        
        # Reporte de clasificaci√≥n
        y_pred_optimal = (best_proba >= self.optimal_threshold).astype(int)
        print(f"\nüìä REPORTE DE CLASIFICACI√ìN:")
        print(classification_report(y_test, y_pred_optimal))
        
        # Importancia de caracter√≠sticas (si es Random Forest)
        if self.model_name == 'random_forest':
            feature_importance = pd.DataFrame({
                'feature': selected_features,
                'importance': self.best_model.feature_importances_
            }).sort_values('importance', ascending=False)
            
            print(f"\nüîç TOP 10 CARACTER√çSTICAS M√ÅS IMPORTANTES:")
            for i, (_, row) in enumerate(feature_importance.head(10).iterrows()):
                print(f"   {i+1}. {row['feature']}: {row['importance']:.4f}")
        
        # Guardar modelo
        model_data = {
            'model': self.best_model,
            'model_name': self.model_name,
            'scaler': self.scaler,
            'feature_selector': self.feature_selector,
            'train_columns': self.train_columns,
            'label_encoders': self.label_encoders,
            'optimal_threshold': self.optimal_threshold,
            'merchant_risk_scores': self.merchant_risk_scores,
            'location_risk_scores': self.location_risk_scores
        }
        
        joblib.dump(model_data, 'advanced_fraud_model.pkl')
        print(f"\nüíæ Modelo avanzado guardado exitosamente")
        
        return best_score, self.optimal_threshold
    
    def _optimize_threshold(self, y_true, y_proba):
        """üß† Optimizaci√≥n inteligente del umbral de decisi√≥n"""
        from sklearn.metrics import precision_recall_curve
        
        precision, recall, thresholds = precision_recall_curve(y_true, y_proba)
        
        # Calcular F1-score para cada umbral
        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)
        
        # Encontrar umbral que maximiza F1-score
        optimal_idx = np.argmax(f1_scores)
        optimal_threshold = thresholds[optimal_idx] if optimal_idx < len(thresholds) else 0.4
        
        # Ajustar umbral para ser m√°s realista
        if optimal_threshold > 0.7:
            optimal_threshold = 0.6
            print(f"üîß Ajustando umbral alto a 0.6 para predicciones m√°s realistas")
        elif optimal_threshold < 0.2:
            optimal_threshold = 0.3
            print(f"üîß Ajustando umbral bajo a 0.3 para evitar demasiados falsos positivos")
        
        print(f"üéØ Umbral optimizado: {optimal_threshold:.3f}")
        return optimal_threshold
    
    def predict_batch(self, transactions_data):
        """üß† Predicci√≥n para m√∫ltiples transacciones aplicando todas las transformaciones"""
        if not self.best_model or self.train_columns is None:
            print("‚ùå Error: el modelo de IA no ha sido entrenado.")
            return [], []

        try:
            print(f"üß† Iniciando an√°lisis batch con IA s√∫per avanzada...")
            
            # Preparar datos b√°sicos
            df = self.prepare_data(transactions_data)
            
            if df.empty:
                print("‚ùå Error: No se pudieron procesar los datos.")
                return [], []
            
            # Aplicar ingenier√≠a de caracter√≠sticas (igual que en entrenamiento)
            X = self._advanced_feature_engineering(df)
            
            # Aplicar selecci√≥n de caracter√≠sticas (igual que en entrenamiento)
            if hasattr(self, 'feature_selector') and self.feature_selector:
                X_selected = self.feature_selector.transform(X)
            else:
                X_selected = X[self.train_columns] if self.train_columns is not None else X
                
            # Aplicar escalado (igual que en entrenamiento)
            if hasattr(self, 'scaler') and self.scaler:
                X_scaled = self.scaler.transform(X_selected)
            else:
                X_scaled = X_selected
            
            # Hacer predicciones
            predictions = self.best_model.predict(X_scaled)
            probabilities = self.best_model.predict_proba(X_scaled)
            
            # Aplicar umbral optimizado si est√° disponible
            if hasattr(self, 'optimal_threshold') and self.optimal_threshold:
                fraud_probs = probabilities[:, 1] if probabilities.shape[1] > 1 else probabilities[:, 0]
                final_predictions = fraud_probs >= self.optimal_threshold
            else:
                final_predictions = predictions
                
            print(f"üéØ Procesadas {len(final_predictions)} transacciones")
            
            return final_predictions.tolist(), probabilities.tolist()
            
        except Exception as e:
            print(f"‚ùå Error en an√°lisis batch: {e}")
            import traceback
            traceback.print_exc()
            return [], []

    def predict_single_transaction(self, new_transactions):
        """üß† Predicci√≥n inteligente con an√°lisis avanzado para transacciones individuales"""
        try:
            predictions, probabilities = self.predict_batch(new_transactions)
            if predictions and probabilities:
                fraud_prob = probabilities[0][1] if len(probabilities[0]) > 1 else probabilities[0][0]
                return bool(predictions[0]), float(fraud_prob)
            return False, 0.0
        except Exception as e:
            print(f"‚ùå Error en predicci√≥n individual: {e}")
            return False, 0.0
    
    def load_model(self, model_path='advanced_fraud_model.pkl'):
        """üß† Carga modelo pre-entrenado"""
        try:
            model_data = joblib.load(model_path)
            self.best_model = model_data['model']
            self.model_name = model_data['model_name']
            self.scaler = model_data['scaler']
            self.feature_selector = model_data['feature_selector']
            self.train_columns = model_data['train_columns']
            self.label_encoders = model_data['label_encoders']
            self.optimal_threshold = model_data['optimal_threshold']
            print("‚úÖ Modelo avanzado cargado exitosamente")
            return True
        except Exception as e:
            print(f"‚ùå Error al cargar modelo: {e}")
            return False
